{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaust.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1_IVLS4Pkva7ldMViPjft8Mc0EZFlQaKN",
      "authorship_tag": "ABX9TyODy2+XVjBt/IXDzGFarDDr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmghaly/arabic_tools/blob/master/kaust.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_0jtVyfaKkJ",
        "outputId": "61f295c3-2252-43e6-b423-ba83fda07237"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "cwd='/content/drive/MyDrive/ar_challenges' #directory where we keep the data\n",
        "os.chdir(cwd)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIFgMDLFkM__",
        "outputId": "6cfa7173-4e93-4e35-a128-12b0e74d8368"
      },
      "source": [
        "#Clone the github codebase\n",
        "!rm -r code_utils #first delete the code_utils folder, then clone from the codebase (very crude till we figure out using github from colab)\n",
        "!git clone https://github.com/hmghaly/word_align.git code_utils\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'code_utils'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 240 (delta 30), reused 0 (delta 0), pack-reused 186\u001b[K\n",
            "Receiving objects: 100% (240/240), 965.81 KiB | 6.57 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrKfYou8aL8K"
      },
      "source": [
        "#Get the training data\n",
        "!wget https://wti-solveml.kaust.edu.sa/Uploads/DataSets/496b0bb7-4c7a-4831-8708-026918cea2b5/csv\n",
        "#!wget https://wti-solveml.kaust.edu.sa/Uploads/DataSets/684ff648-a1ae-42a4-ba3b-daba98467105/csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXnFo-T_BayX",
        "outputId": "61fb799e-4e7a-466b-f502-ca83460f436f"
      },
      "source": [
        "#Step 1 -\n",
        "#Create BERT feature extraction function\n",
        "!pip install transformers==3.1.0\n",
        "import torch\n",
        "import transformers\n",
        "import itertools\n",
        "\n",
        "bert_model = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "align_layer=8\n",
        "test_sent=\"وأنا راجع رحت أجيب الخضار\"\n",
        "sent_src=test_sent.strip().split()\n",
        "token_src= [tokenizer.tokenize(word) for word in sent_src] #to use the bert tokenizer\n",
        "#print(token_src)\n",
        "#token_src= [[word] for word in sent_src] #just to use the original tokenization\n",
        "wid_src= [tokenizer.convert_tokens_to_ids(x) for x in token_src]\n",
        "ids_src = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids']\n",
        "sent_bert_features = bert_model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "\n",
        "#sent_bert_features=sent2bert(test_sent)\n",
        "print(sent_bert_features.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "torch.Size([13, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3gmi0X0FZU4",
        "outputId": "d1841d60-459d-47f0-f085-c4d823e9feb7"
      },
      "source": [
        "#Step 2-\n",
        "#create an RNN/LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector \n",
        "    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)\n",
        "    self.hidden2out = nn.Linear(hidden_size, output_size)\n",
        "    self.hidden = self.init_hidden()\n",
        "  def forward(self, feature_list):\n",
        "    feature_list.to(device) #### <<<<<<<<<<<<<<<<< \n",
        "    if self.matching_in_out:\n",
        "      # test=feature_list.view(len( feature_list), 1, -1)\n",
        "      # print(test.shape)\n",
        "      lstm_out, _ = self.lstm( feature_list.view(len( feature_list), 1, -1))\n",
        "      output_space = self.hidden2out(lstm_out.view(len( feature_list), -1))\n",
        "      output_scores = torch.sigmoid(output_space) #we'll need to check if we need this sigmoid\n",
        "      return output_scores #output_scores\n",
        "    else:\n",
        "      for i in range(len(feature_list)):\n",
        "        cur_ft_tensor=feature_list[i]#.view([1,1,self.input_size])\n",
        "        cur_ft_tensor=cur_ft_tensor.view([1,1,self.input_size])\n",
        "        lstm_out, self.hidden = self.lstm(cur_ft_tensor, self.hidden)\n",
        "        outs=self.hidden2out(lstm_out)\n",
        "      return outs\n",
        "  def init_hidden(self):\n",
        "    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)\n",
        "    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device),\n",
        "            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device))\n",
        "\n",
        "# test_sent=\"الناس دول مش عارفين بيعملوا إيه\"\n",
        "# sent_bert_features=sent2bert(test_sent)\n",
        "# print(sent_bert_features.shape)\n",
        "\n",
        "n_input=768\n",
        "n_output=3\n",
        "n_hidden=64\n",
        "n_layers=3\n",
        "io_matching=False\n",
        "\n",
        "if io_matching:\n",
        "  print(\"input size matches output size\")\n",
        "else:\n",
        "  print(\"output size is fixed by n_output\")\n",
        "\n",
        "\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,io_matching)\n",
        "rnn.to(device)\n",
        "# sent_bert_features=sent_bert_features.to(device)\n",
        "# print(sent_bert_features)\n",
        "sent_bert_features=torch.rand((15, n_input)).to(device)\n",
        "output = rnn(sent_bert_features)\n",
        "output.to(device)\n",
        "print(\"output.shape:\", output.shape)\n",
        "print(\"output:\", output.view([n_output]))\n",
        "print(rnn)\n",
        "\n",
        "# features=[[0.0552, 0.5666, 0.0039, 0.0709, 0.9022, 0.2247],\n",
        "#         [0.4827, 0.5223, 0.9601, 0.6596, 0.7441, 0.2861],\n",
        "#         [0.3286, 0.8637, 0.6833, 0.6254, 0.1229, 0.1010],\n",
        "#         [0.6010, 0.1248, 0.8506, 0.9166, 0.2929, 0.1777],\n",
        "#         [0.9840, 0.2332, 0.1015, 0.1371, 0.5869, 0.5971]]\n",
        "\n",
        "# #feature_tensor=torch.tensor(features)\n",
        "\n",
        "# # print(\"shape of feature tensor\", feature_tensor.shape)\n",
        "# # print(feature_tensor)\n",
        "# feature_tensor=torch.rand((15, n_input))\n",
        "# # print(\"feature_tensor:\", feature_tensor.shape)\n",
        "# print(feature_tensor)\n",
        "# #line_tensor=torch.rand((10, 4))\n",
        "# output = rnn(sent_bert_features)\n",
        "# print(\"output.shape:\", output.shape)\n",
        "# print(\"output:\", output.view([n_output]))\n",
        "\n",
        "# print(rnn)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output size is fixed by n_output\n",
            "output.shape: torch.Size([1, 1, 3])\n",
            "output: tensor([-0.0573,  0.0137, -0.0831], device='cuda:0', grad_fn=<ViewBackward>)\n",
            "RNN(\n",
            "  (lstm): LSTM(768, 64, num_layers=3)\n",
            "  (hidden2out): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C5b35k4Tb-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2a98d2-5d8a-4b5c-cb80-d0d02b5d6aae"
      },
      "source": [
        "#Step 3- Now defining the functions we will need\n",
        "import os\n",
        "from code_utils.align_utils import *\n",
        "from code_utils.arabic_lib import *\n",
        "from code_utils.general import * \n",
        "from itertools import groupby\n",
        "from collections import Counter\n",
        "import h5py\n",
        "\n",
        "\n",
        "def sent2bert(sent):\n",
        "  align_layer = 8\n",
        "  sent_src=sent.strip().split()\n",
        "  token_src= [tokenizer.tokenize(word) for word in sent_src] #to use the bert tokenizer\n",
        "  #print(token_src)\n",
        "  #token_src= [[word] for word in sent_src] #just to use the original tokenization\n",
        "  wid_src= [tokenizer.convert_tokens_to_ids(x) for x in token_src]\n",
        "  ids_src = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids']\n",
        "  out_src = bert_model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "  return out_src\n",
        "\n",
        "def feature_extraction_full(raw_sent):\n",
        "  sent_str=raw_sent.replace(\"<br>\",\" \")\n",
        "  sent_str=clean_ar(sent_str)\n",
        "  sent_toks=tok(sent_str)\n",
        "  sent_toks_str=\" \".join(sent_toks)\n",
        "  feature_tensor=sent2bert(sent_toks_str)\n",
        "  return feature_tensor\n",
        "\n",
        "import gensim\n",
        "aravec_model = gensim.models.Word2Vec.load('full_uni_cbow_300_twitter.mdl')\n",
        "def feature_extraction_wv(raw_sent,wv_model):\n",
        "  sent_str=raw_sent.replace(\"<br>\",\" \")\n",
        "  sent_str=clean_ar(sent_str)\n",
        "  sent_toks=tok(sent_str)\n",
        "  for tok0 in sent_toks:\n",
        "    toc_vector=wv_model[tok0]\n",
        "    print(tok0,)  \n",
        "\n",
        "\n",
        "ar_counter_dict=load_counts(\"ar_count.txt\", tmp_count_dict={})\n",
        "\n",
        "def outcome2tensor(cur_outcome_label,all_outcome_labels):\n",
        "  initial_list=[0.]*len(all_outcome_labels)\n",
        "  initial_list[all_outcome_labels.index(cur_outcome_label)]=1.\n",
        "  return torch.tensor(initial_list)\n",
        "\n",
        "\n",
        "def pred2label(output_tensor,label_list):\n",
        "  output_tensor=output_tensor.view([len(label_list)])\n",
        "  output_list=output_tensor.tolist()\n",
        "  #print(output_list, output_tensor, output_tensor.shape)\n",
        "  pred_list=[(lb,output_list[i_]) for i_,lb in enumerate(label_list)]\n",
        "  pred_list.sort(key=lambda x:-x[1])\n",
        "  return pred_list\n",
        "\n",
        "def pred_correctness_dist(full_preds_list):\n",
        "  pred_dist=[]\n",
        "  full_preds_list.sort()\n",
        "  for key, group in groupby(full_preds_list,lambda x:x[0]):\n",
        "    #print(key,Counter([v[1] for v in list(group)])) \n",
        "    pred_dist.append((key,Counter([v[1] for v in list(group)])))\n",
        "  return pred_dist\n",
        "\n",
        "print(\"loaded the functions for feature and outcome processing\")\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded the functions for feature and outcome processing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUX6cJWehGH2",
        "outputId": "20ac49c6-7bcc-4159-83c2-92b89bc7e2a7"
      },
      "source": [
        "#Step 4 - \n",
        "#now get the data and split to training and dev sets\n",
        "#use both the WTI training and sarcasm training for a larger datase\n",
        "\n",
        "import h5py\n",
        "#hdf5_fpath=os.path.join(exp_dir,\"data.hdf5\")\n",
        "hdf5_fpath=\"data.hdf5\"\n",
        "hdf5_file=h5py.File(hdf5_fpath, 'w')\n",
        "train_grp = hdf5_file.create_group(\"train\")\n",
        "sarcasm_grp = hdf5_file.create_group(\"sarcasm\")\n",
        "test_grp = hdf5_file.create_group(\"test\")\n",
        "   # for item in hdf5_file: \n",
        "    #     obj=hdf5_file[item]\n",
        "    #     for ds in obj:\n",
        "    #         cur_data=obj[ds]\n",
        "    #         print(item, ds, cur_data.shape, cur_data.attrs[\"uas\"])\n",
        "        #print(item)\n",
        "# dset = grp.create_dataset(example_id, data=one_hot_tensor_torch, compression=\"gzip\")\n",
        "# dset.attrs[\"uas\"]=parser_uas\n",
        "# dset.attrs[\"sent_size\"]=sent_size        \n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "#first the WTI training\n",
        "full_training_list=[]\n",
        "training_fopen=open(\"combined_training.tsv\")\n",
        "for i0,tf_line in enumerate(training_fopen):\n",
        "  if i0%100==0: print(\"training item:\", i0)\n",
        "  line_split=tf_line.strip(\"\\n\\r\\t\").split(\"\\t\")\n",
        "  if len(line_split)!=3: continue\n",
        "  tweet_id,sentiment_str,tweet_text=line_split\n",
        "  if sentiment_str.lower()==\"positive\": sentiment=1\n",
        "  elif sentiment_str.lower()==\"negative\": sentiment=-1\n",
        "  else: sentiment=0\n",
        "  feature_tensor=feature_extraction_full(tweet_text)\n",
        "  ft_numpy=feature_tensor.detach().numpy()\n",
        "  dset = train_grp.create_dataset(str(tweet_id), data=ft_numpy, compression=\"gzip\")\n",
        "  dset.attrs[\"text\"]=tweet_text\n",
        "  dset.attrs[\"sentiment\"]=sentiment        \n",
        "\n",
        "  full_training_list.append((tweet_text,sentiment))\n",
        "  #if i0>20: break\n",
        "#hdf5_file.close()\n",
        "\n",
        "#Now for the data of sarcasm\n",
        "train_df = pd.read_csv(\"ArSarcasm_train.csv\")\n",
        "for i,a in enumerate(train_df.iterrows()):\n",
        "  if i%50==0: print(\"sarcasm item: \",i)\n",
        "  tweet_id=\"sarcasm_%s\"%i\n",
        "  row_dict=dict(iter(a[1].items())) \n",
        "  #print(row_dict)\n",
        "  tweet_text=row_dict[\"tweet\"]\n",
        "  sentiment_str=row_dict[\"sentiment\"] # print(row_dict[\"dialect\"])\n",
        "  if sentiment_str.lower()==\"positive\": sentiment=1\n",
        "  elif sentiment_str.lower()==\"negative\": sentiment=-1\n",
        "  else: sentiment=0\n",
        "  full_training_list.append((tweet_text,sentiment))\n",
        "  feature_tensor=feature_extraction_full(tweet_text)\n",
        "  ft_numpy=feature_tensor.detach().numpy()\n",
        "  dset = sarcasm_grp.create_dataset(tweet_id, data=ft_numpy, compression=\"gzip\")\n",
        "  dset.attrs[\"text\"]=tweet_text\n",
        "  dset.attrs[\"sentiment\"]=sentiment      \n",
        "  #if i>20: break  \n",
        "\n",
        "\n",
        "dev_set=full_training_list[:5000]\n",
        "train_set=full_training_list[5000:]\n",
        "print(\"full training size\", len(full_training_list))\n",
        "print(\"full dev_set\", len(dev_set))\n",
        "print(\"full train_set\", len(train_set))\n",
        "\n",
        "print(\"now doing the test set\")\n",
        "test_set_with_ids=[]\n",
        "test_df = pd.read_csv(\"test1_with_text.csv\")\n",
        "for i,a in enumerate(test_df.iterrows()):\n",
        "  rnn.zero_grad()\n",
        "  if i%20==0: print(\"test item\", i)\n",
        "  row_dict=dict(iter(a[1].items())) \n",
        "  #print(row_dict)\n",
        "  tweet_id=row_dict[\"Tweet_id\"]\n",
        "  tweet_text=row_dict[\"Text\"]\n",
        "  test_set_with_ids.append((tweet_id,tweet_text))\n",
        "  feature_tensor=feature_extraction_full(tweet_text)\n",
        "  ft_numpy=feature_tensor.detach().numpy()\n",
        "  dset = test_grp.create_dataset(str(tweet_id), data=ft_numpy, compression=\"gzip\")\n",
        "  dset.attrs[\"text\"]=tweet_text\n",
        "  #dset.attrs[\"sentiment\"]=sentiment  \n",
        "  #if i>20: break      \n",
        "\n",
        "print(\"test_set_with_ids\",len(test_set_with_ids))\n",
        "hdf5_file.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training item: 0\n",
            "training item: 100\n",
            "training item: 200\n",
            "training item: 300\n",
            "training item: 400\n",
            "training item: 500\n",
            "training item: 600\n",
            "training item: 700\n",
            "training item: 800\n",
            "training item: 900\n",
            "training item: 1000\n",
            "training item: 1100\n",
            "training item: 1200\n",
            "training item: 1300\n",
            "training item: 1400\n",
            "training item: 1500\n",
            "training item: 1600\n",
            "training item: 1700\n",
            "training item: 1800\n",
            "training item: 1900\n",
            "training item: 2000\n",
            "training item: 2100\n",
            "training item: 2200\n",
            "training item: 2300\n",
            "training item: 2400\n",
            "training item: 2500\n",
            "training item: 2600\n",
            "training item: 2700\n",
            "training item: 2800\n",
            "training item: 2900\n",
            "training item: 3000\n",
            "training item: 3100\n",
            "training item: 3200\n",
            "training item: 3300\n",
            "training item: 3400\n",
            "training item: 3500\n",
            "training item: 3600\n",
            "training item: 3700\n",
            "training item: 3800\n",
            "training item: 3900\n",
            "training item: 4000\n",
            "training item: 4100\n",
            "training item: 4200\n",
            "training item: 4300\n",
            "training item: 4400\n",
            "training item: 4500\n",
            "training item: 4600\n",
            "training item: 4700\n",
            "training item: 4800\n",
            "training item: 4900\n",
            "training item: 5000\n",
            "training item: 5100\n",
            "training item: 5200\n",
            "training item: 5300\n",
            "training item: 5400\n",
            "training item: 5500\n",
            "training item: 5600\n",
            "training item: 5700\n",
            "training item: 5800\n",
            "training item: 5900\n",
            "training item: 6000\n",
            "training item: 6100\n",
            "training item: 6200\n",
            "training item: 6300\n",
            "training item: 6400\n",
            "training item: 6500\n",
            "training item: 6600\n",
            "training item: 6700\n",
            "training item: 6800\n",
            "training item: 6900\n",
            "training item: 7000\n",
            "training item: 7100\n",
            "training item: 7200\n",
            "training item: 7300\n",
            "training item: 7400\n",
            "training item: 7500\n",
            "training item: 7600\n",
            "training item: 7700\n",
            "training item: 7800\n",
            "training item: 7900\n",
            "training item: 8000\n",
            "training item: 8100\n",
            "training item: 8200\n",
            "training item: 8300\n",
            "training item: 8400\n",
            "training item: 8500\n",
            "training item: 8600\n",
            "training item: 8700\n",
            "training item: 8800\n",
            "training item: 8900\n",
            "training item: 9000\n",
            "training item: 9100\n",
            "training item: 9200\n",
            "training item: 9300\n",
            "training item: 9400\n",
            "training item: 9500\n",
            "training item: 9600\n",
            "training item: 9700\n",
            "training item: 9800\n",
            "training item: 9900\n",
            "training item: 10000\n",
            "training item: 10100\n",
            "training item: 10200\n",
            "training item: 10300\n",
            "training item: 10400\n",
            "training item: 10500\n",
            "training item: 10600\n",
            "training item: 10700\n",
            "training item: 10800\n",
            "training item: 10900\n",
            "training item: 11000\n",
            "training item: 11100\n",
            "training item: 11200\n",
            "training item: 11300\n",
            "training item: 11400\n",
            "training item: 11500\n",
            "training item: 11600\n",
            "training item: 11700\n",
            "training item: 11800\n",
            "training item: 11900\n",
            "training item: 12000\n",
            "training item: 12100\n",
            "training item: 12200\n",
            "training item: 12300\n",
            "training item: 12400\n",
            "training item: 12500\n",
            "training item: 12600\n",
            "training item: 12700\n",
            "training item: 12800\n",
            "training item: 12900\n",
            "training item: 13000\n",
            "training item: 13100\n",
            "training item: 13200\n",
            "training item: 13300\n",
            "training item: 13400\n",
            "training item: 13500\n",
            "training item: 13600\n",
            "training item: 13700\n",
            "training item: 13800\n",
            "training item: 13900\n",
            "training item: 14000\n",
            "training item: 14100\n",
            "training item: 14200\n",
            "training item: 14300\n",
            "training item: 14400\n",
            "training item: 14500\n",
            "training item: 14600\n",
            "training item: 14700\n",
            "training item: 14800\n",
            "training item: 14900\n",
            "training item: 15000\n",
            "training item: 15100\n",
            "training item: 15200\n",
            "training item: 15300\n",
            "training item: 15400\n",
            "training item: 15500\n",
            "training item: 15600\n",
            "training item: 15700\n",
            "training item: 15800\n",
            "training item: 15900\n",
            "training item: 16000\n",
            "training item: 16100\n",
            "training item: 16200\n",
            "training item: 16300\n",
            "training item: 16400\n",
            "training item: 16500\n",
            "training item: 16600\n",
            "training item: 16700\n",
            "training item: 16800\n",
            "training item: 16900\n",
            "training item: 17000\n",
            "training item: 17100\n",
            "training item: 17200\n",
            "training item: 17300\n",
            "training item: 17400\n",
            "training item: 17500\n",
            "training item: 17600\n",
            "training item: 17700\n",
            "training item: 17800\n",
            "training item: 17900\n",
            "training item: 18000\n",
            "training item: 18100\n",
            "training item: 18200\n",
            "training item: 18300\n",
            "training item: 18400\n",
            "training item: 18500\n",
            "training item: 18600\n",
            "training item: 18700\n",
            "training item: 18800\n",
            "training item: 18900\n",
            "training item: 19000\n",
            "training item: 19100\n",
            "training item: 19200\n",
            "training item: 19300\n",
            "training item: 19400\n",
            "training item: 19500\n",
            "training item: 19600\n",
            "training item: 19700\n",
            "training item: 19800\n",
            "training item: 19900\n",
            "training item: 20000\n",
            "training item: 20100\n",
            "training item: 20200\n",
            "training item: 20300\n",
            "training item: 20400\n",
            "training item: 20500\n",
            "training item: 20600\n",
            "training item: 20700\n",
            "training item: 20800\n",
            "training item: 20900\n",
            "training item: 21000\n",
            "training item: 21100\n",
            "training item: 21200\n",
            "training item: 21300\n",
            "training item: 21400\n",
            "training item: 21500\n",
            "training item: 21600\n",
            "training item: 21700\n",
            "training item: 21800\n",
            "training item: 21900\n",
            "training item: 22000\n",
            "training item: 22100\n",
            "training item: 22200\n",
            "training item: 22300\n",
            "training item: 22400\n",
            "training item: 22500\n",
            "training item: 22600\n",
            "training item: 22700\n",
            "training item: 22800\n",
            "training item: 22900\n",
            "training item: 23000\n",
            "training item: 23100\n",
            "training item: 23200\n",
            "training item: 23300\n",
            "training item: 23400\n",
            "training item: 23500\n",
            "training item: 23600\n",
            "training item: 23700\n",
            "training item: 23800\n",
            "training item: 23900\n",
            "training item: 24000\n",
            "training item: 24100\n",
            "training item: 24200\n",
            "training item: 24300\n",
            "training item: 24400\n",
            "training item: 24500\n",
            "training item: 24600\n",
            "training item: 24700\n",
            "training item: 24800\n",
            "training item: 24900\n",
            "training item: 25000\n",
            "training item: 25100\n",
            "training item: 25200\n",
            "training item: 25300\n",
            "training item: 25400\n",
            "training item: 25500\n",
            "training item: 25600\n",
            "training item: 25700\n",
            "training item: 25800\n",
            "training item: 25900\n",
            "training item: 26000\n",
            "training item: 26100\n",
            "training item: 26200\n",
            "training item: 26300\n",
            "training item: 26400\n",
            "training item: 26500\n",
            "training item: 26600\n",
            "training item: 26700\n",
            "training item: 26800\n",
            "training item: 26900\n",
            "training item: 27000\n",
            "training item: 27100\n",
            "training item: 27200\n",
            "training item: 27300\n",
            "training item: 27400\n",
            "training item: 27500\n",
            "training item: 27600\n",
            "training item: 27700\n",
            "training item: 27800\n",
            "training item: 27900\n",
            "training item: 28000\n",
            "training item: 28100\n",
            "training item: 28200\n",
            "training item: 28300\n",
            "training item: 28400\n",
            "training item: 28500\n",
            "training item: 28600\n",
            "training item: 28700\n",
            "training item: 28800\n",
            "training item: 28900\n",
            "training item: 29000\n",
            "training item: 29100\n",
            "training item: 29200\n",
            "training item: 29300\n",
            "training item: 29400\n",
            "training item: 29500\n",
            "training item: 29600\n",
            "training item: 29700\n",
            "training item: 29800\n",
            "training item: 29900\n",
            "training item: 30000\n",
            "training item: 30100\n",
            "training item: 30200\n",
            "training item: 30300\n",
            "training item: 30400\n",
            "training item: 30500\n",
            "training item: 30600\n",
            "training item: 30700\n",
            "training item: 30800\n",
            "training item: 30900\n",
            "training item: 31000\n",
            "training item: 31100\n",
            "training item: 31200\n",
            "training item: 31300\n",
            "training item: 31400\n",
            "training item: 31500\n",
            "training item: 31600\n",
            "training item: 31700\n",
            "training item: 31800\n",
            "training item: 31900\n",
            "training item: 32000\n",
            "training item: 32100\n",
            "training item: 32200\n",
            "training item: 32300\n",
            "training item: 32400\n",
            "training item: 32500\n",
            "training item: 32600\n",
            "training item: 32700\n",
            "training item: 32800\n",
            "training item: 32900\n",
            "training item: 33000\n",
            "training item: 33100\n",
            "training item: 33200\n",
            "sarcasm item:  0\n",
            "sarcasm item:  50\n",
            "sarcasm item:  100\n",
            "sarcasm item:  150\n",
            "sarcasm item:  200\n",
            "sarcasm item:  250\n",
            "sarcasm item:  300\n",
            "sarcasm item:  350\n",
            "sarcasm item:  400\n",
            "sarcasm item:  450\n",
            "sarcasm item:  500\n",
            "sarcasm item:  550\n",
            "sarcasm item:  600\n",
            "sarcasm item:  650\n",
            "sarcasm item:  700\n",
            "sarcasm item:  750\n",
            "sarcasm item:  800\n",
            "sarcasm item:  850\n",
            "sarcasm item:  900\n",
            "sarcasm item:  950\n",
            "sarcasm item:  1000\n",
            "sarcasm item:  1050\n",
            "sarcasm item:  1100\n",
            "sarcasm item:  1150\n",
            "sarcasm item:  1200\n",
            "sarcasm item:  1250\n",
            "sarcasm item:  1300\n",
            "sarcasm item:  1350\n",
            "sarcasm item:  1400\n",
            "sarcasm item:  1450\n",
            "sarcasm item:  1500\n",
            "sarcasm item:  1550\n",
            "sarcasm item:  1600\n",
            "sarcasm item:  1650\n",
            "sarcasm item:  1700\n",
            "sarcasm item:  1750\n",
            "sarcasm item:  1800\n",
            "sarcasm item:  1850\n",
            "sarcasm item:  1900\n",
            "sarcasm item:  1950\n",
            "sarcasm item:  2000\n",
            "sarcasm item:  2050\n",
            "sarcasm item:  2100\n",
            "sarcasm item:  2150\n",
            "sarcasm item:  2200\n",
            "sarcasm item:  2250\n",
            "sarcasm item:  2300\n",
            "sarcasm item:  2350\n",
            "sarcasm item:  2400\n",
            "sarcasm item:  2450\n",
            "sarcasm item:  2500\n",
            "sarcasm item:  2550\n",
            "sarcasm item:  2600\n",
            "sarcasm item:  2650\n",
            "sarcasm item:  2700\n",
            "sarcasm item:  2750\n",
            "sarcasm item:  2800\n",
            "sarcasm item:  2850\n",
            "sarcasm item:  2900\n",
            "sarcasm item:  2950\n",
            "sarcasm item:  3000\n",
            "sarcasm item:  3050\n",
            "sarcasm item:  3100\n",
            "sarcasm item:  3150\n",
            "sarcasm item:  3200\n",
            "sarcasm item:  3250\n",
            "sarcasm item:  3300\n",
            "sarcasm item:  3350\n",
            "sarcasm item:  3400\n",
            "sarcasm item:  3450\n",
            "sarcasm item:  3500\n",
            "sarcasm item:  3550\n",
            "sarcasm item:  3600\n",
            "sarcasm item:  3650\n",
            "sarcasm item:  3700\n",
            "sarcasm item:  3750\n",
            "sarcasm item:  3800\n",
            "sarcasm item:  3850\n",
            "sarcasm item:  3900\n",
            "sarcasm item:  3950\n",
            "sarcasm item:  4000\n",
            "sarcasm item:  4050\n",
            "sarcasm item:  4100\n",
            "sarcasm item:  4150\n",
            "sarcasm item:  4200\n",
            "sarcasm item:  4250\n",
            "sarcasm item:  4300\n",
            "sarcasm item:  4350\n",
            "sarcasm item:  4400\n",
            "sarcasm item:  4450\n",
            "sarcasm item:  4500\n",
            "sarcasm item:  4550\n",
            "sarcasm item:  4600\n",
            "sarcasm item:  4650\n",
            "sarcasm item:  4700\n",
            "sarcasm item:  4750\n",
            "sarcasm item:  4800\n",
            "sarcasm item:  4850\n",
            "sarcasm item:  4900\n",
            "sarcasm item:  4950\n",
            "sarcasm item:  5000\n",
            "sarcasm item:  5050\n",
            "sarcasm item:  5100\n",
            "sarcasm item:  5150\n",
            "sarcasm item:  5200\n",
            "sarcasm item:  5250\n",
            "sarcasm item:  5300\n",
            "sarcasm item:  5350\n",
            "sarcasm item:  5400\n",
            "sarcasm item:  5450\n",
            "sarcasm item:  5500\n",
            "sarcasm item:  5550\n",
            "sarcasm item:  5600\n",
            "sarcasm item:  5650\n",
            "sarcasm item:  5700\n",
            "sarcasm item:  5750\n",
            "sarcasm item:  5800\n",
            "sarcasm item:  5850\n",
            "sarcasm item:  5900\n",
            "sarcasm item:  5950\n",
            "sarcasm item:  6000\n",
            "sarcasm item:  6050\n",
            "sarcasm item:  6100\n",
            "sarcasm item:  6150\n",
            "sarcasm item:  6200\n",
            "sarcasm item:  6250\n",
            "sarcasm item:  6300\n",
            "sarcasm item:  6350\n",
            "sarcasm item:  6400\n",
            "sarcasm item:  6450\n",
            "sarcasm item:  6500\n",
            "sarcasm item:  6550\n",
            "sarcasm item:  6600\n",
            "sarcasm item:  6650\n",
            "sarcasm item:  6700\n",
            "sarcasm item:  6750\n",
            "sarcasm item:  6800\n",
            "sarcasm item:  6850\n",
            "sarcasm item:  6900\n",
            "sarcasm item:  6950\n",
            "sarcasm item:  7000\n",
            "sarcasm item:  7050\n",
            "sarcasm item:  7100\n",
            "sarcasm item:  7150\n",
            "sarcasm item:  7200\n",
            "sarcasm item:  7250\n",
            "sarcasm item:  7300\n",
            "sarcasm item:  7350\n",
            "sarcasm item:  7400\n",
            "sarcasm item:  7450\n",
            "sarcasm item:  7500\n",
            "sarcasm item:  7550\n",
            "sarcasm item:  7600\n",
            "sarcasm item:  7650\n",
            "sarcasm item:  7700\n",
            "sarcasm item:  7750\n",
            "sarcasm item:  7800\n",
            "sarcasm item:  7850\n",
            "sarcasm item:  7900\n",
            "sarcasm item:  7950\n",
            "sarcasm item:  8000\n",
            "sarcasm item:  8050\n",
            "sarcasm item:  8100\n",
            "sarcasm item:  8150\n",
            "sarcasm item:  8200\n",
            "sarcasm item:  8250\n",
            "sarcasm item:  8300\n",
            "sarcasm item:  8350\n",
            "sarcasm item:  8400\n",
            "full training size 41646\n",
            "full dev_set 5000\n",
            "full train_set 36646\n",
            "now doing the test set\n",
            "test item 0\n",
            "test item 20\n",
            "test item 40\n",
            "test item 60\n",
            "test item 80\n",
            "test item 100\n",
            "test item 120\n",
            "test item 140\n",
            "test item 160\n",
            "test item 180\n",
            "test item 200\n",
            "test item 220\n",
            "test item 240\n",
            "test item 260\n",
            "test item 280\n",
            "test item 300\n",
            "test item 320\n",
            "test item 340\n",
            "test item 360\n",
            "test item 380\n",
            "test item 400\n",
            "test item 420\n",
            "test item 440\n",
            "test item 460\n",
            "test item 480\n",
            "test item 500\n",
            "test item 520\n",
            "test item 540\n",
            "test item 560\n",
            "test item 580\n",
            "test item 600\n",
            "test item 620\n",
            "test item 640\n",
            "test item 660\n",
            "test item 680\n",
            "test item 700\n",
            "test item 720\n",
            "test item 740\n",
            "test item 760\n",
            "test item 780\n",
            "test item 800\n",
            "test item 820\n",
            "test item 840\n",
            "test item 860\n",
            "test item 880\n",
            "test item 900\n",
            "test item 920\n",
            "test item 940\n",
            "test item 960\n",
            "test item 980\n",
            "test item 1000\n",
            "test item 1020\n",
            "test item 1040\n",
            "test item 1060\n",
            "test item 1080\n",
            "test item 1100\n",
            "test item 1120\n",
            "test item 1140\n",
            "test item 1160\n",
            "test item 1180\n",
            "test item 1200\n",
            "test item 1220\n",
            "test item 1240\n",
            "test item 1260\n",
            "test item 1280\n",
            "test item 1300\n",
            "test item 1320\n",
            "test item 1340\n",
            "test item 1360\n",
            "test item 1380\n",
            "test item 1400\n",
            "test item 1420\n",
            "test item 1440\n",
            "test item 1460\n",
            "test item 1480\n",
            "test item 1500\n",
            "test item 1520\n",
            "test item 1540\n",
            "test item 1560\n",
            "test item 1580\n",
            "test item 1600\n",
            "test item 1620\n",
            "test item 1640\n",
            "test item 1660\n",
            "test item 1680\n",
            "test item 1700\n",
            "test item 1720\n",
            "test item 1740\n",
            "test item 1760\n",
            "test item 1780\n",
            "test item 1800\n",
            "test item 1820\n",
            "test item 1840\n",
            "test item 1860\n",
            "test item 1880\n",
            "test item 1900\n",
            "test item 1920\n",
            "test item 1940\n",
            "test item 1960\n",
            "test item 1980\n",
            "test item 2000\n",
            "test item 2020\n",
            "test item 2040\n",
            "test item 2060\n",
            "test item 2080\n",
            "test item 2100\n",
            "test item 2120\n",
            "test item 2140\n",
            "test item 2160\n",
            "test item 2180\n",
            "test item 2200\n",
            "test item 2220\n",
            "test item 2240\n",
            "test item 2260\n",
            "test item 2280\n",
            "test item 2300\n",
            "test item 2320\n",
            "test item 2340\n",
            "test item 2360\n",
            "test item 2380\n",
            "test item 2400\n",
            "test item 2420\n",
            "test item 2440\n",
            "test item 2460\n",
            "test item 2480\n",
            "test item 2500\n",
            "test item 2520\n",
            "test item 2540\n",
            "test item 2560\n",
            "test item 2580\n",
            "test item 2600\n",
            "test item 2620\n",
            "test item 2640\n",
            "test item 2660\n",
            "test item 2680\n",
            "test item 2700\n",
            "test item 2720\n",
            "test item 2740\n",
            "test item 2760\n",
            "test item 2780\n",
            "test item 2800\n",
            "test item 2820\n",
            "test item 2840\n",
            "test item 2860\n",
            "test item 2880\n",
            "test item 2900\n",
            "test item 2920\n",
            "test item 2940\n",
            "test item 2960\n",
            "test item 2980\n",
            "test item 3000\n",
            "test item 3020\n",
            "test item 3040\n",
            "test item 3060\n",
            "test item 3080\n",
            "test item 3100\n",
            "test item 3120\n",
            "test item 3140\n",
            "test item 3160\n",
            "test item 3180\n",
            "test item 3200\n",
            "test item 3220\n",
            "test item 3240\n",
            "test item 3260\n",
            "test item 3280\n",
            "test item 3300\n",
            "test item 3320\n",
            "test item 3340\n",
            "test item 3360\n",
            "test item 3380\n",
            "test item 3400\n",
            "test item 3420\n",
            "test item 3440\n",
            "test item 3460\n",
            "test item 3480\n",
            "test item 3500\n",
            "test item 3520\n",
            "test item 3540\n",
            "test item 3560\n",
            "test item 3580\n",
            "test item 3600\n",
            "test item 3620\n",
            "test item 3640\n",
            "test item 3660\n",
            "test item 3680\n",
            "test item 3700\n",
            "test item 3720\n",
            "test item 3740\n",
            "test item 3760\n",
            "test item 3780\n",
            "test item 3800\n",
            "test item 3820\n",
            "test item 3840\n",
            "test item 3860\n",
            "test item 3880\n",
            "test item 3900\n",
            "test item 3920\n",
            "test item 3940\n",
            "test item 3960\n",
            "test item 3980\n",
            "test item 4000\n",
            "test item 4020\n",
            "test item 4040\n",
            "test item 4060\n",
            "test item 4080\n",
            "test item 4100\n",
            "test item 4120\n",
            "test item 4140\n",
            "test item 4160\n",
            "test item 4180\n",
            "test item 4200\n",
            "test item 4220\n",
            "test item 4240\n",
            "test item 4260\n",
            "test item 4280\n",
            "test item 4300\n",
            "test item 4320\n",
            "test item 4340\n",
            "test item 4360\n",
            "test item 4380\n",
            "test item 4400\n",
            "test item 4420\n",
            "test item 4440\n",
            "test item 4460\n",
            "test item 4480\n",
            "test item 4500\n",
            "test item 4520\n",
            "test item 4540\n",
            "test item 4560\n",
            "test item 4580\n",
            "test item 4600\n",
            "test item 4620\n",
            "test item 4640\n",
            "test item 4660\n",
            "test item 4680\n",
            "test item 4700\n",
            "test item 4720\n",
            "test item 4740\n",
            "test item 4760\n",
            "test item 4780\n",
            "test item 4800\n",
            "test item 4820\n",
            "test item 4840\n",
            "test item 4860\n",
            "test item 4880\n",
            "test item 4900\n",
            "test item 4920\n",
            "test item 4940\n",
            "test item 4960\n",
            "test item 4980\n",
            "test item 5000\n",
            "test item 5020\n",
            "test item 5040\n",
            "test item 5060\n",
            "test item 5080\n",
            "test item 5100\n",
            "test item 5120\n",
            "test item 5140\n",
            "test item 5160\n",
            "test item 5180\n",
            "test item 5200\n",
            "test item 5220\n",
            "test item 5240\n",
            "test item 5260\n",
            "test item 5280\n",
            "test item 5300\n",
            "test item 5320\n",
            "test item 5340\n",
            "test item 5360\n",
            "test item 5380\n",
            "test item 5400\n",
            "test item 5420\n",
            "test item 5440\n",
            "test item 5460\n",
            "test item 5480\n",
            "test item 5500\n",
            "test item 5520\n",
            "test item 5540\n",
            "test item 5560\n",
            "test item 5580\n",
            "test item 5600\n",
            "test item 5620\n",
            "test item 5640\n",
            "test item 5660\n",
            "test item 5680\n",
            "test item 5700\n",
            "test item 5720\n",
            "test item 5740\n",
            "test item 5760\n",
            "test item 5780\n",
            "test item 5800\n",
            "test item 5820\n",
            "test item 5840\n",
            "test item 5860\n",
            "test item 5880\n",
            "test item 5900\n",
            "test item 5920\n",
            "test item 5940\n",
            "test item 5960\n",
            "test item 5980\n",
            "test item 6000\n",
            "test item 6020\n",
            "test item 6040\n",
            "test item 6060\n",
            "test item 6080\n",
            "test item 6100\n",
            "test item 6120\n",
            "test item 6140\n",
            "test item 6160\n",
            "test item 6180\n",
            "test item 6200\n",
            "test item 6220\n",
            "test item 6240\n",
            "test item 6260\n",
            "test item 6280\n",
            "test item 6300\n",
            "test item 6320\n",
            "test item 6340\n",
            "test item 6360\n",
            "test item 6380\n",
            "test item 6400\n",
            "test item 6420\n",
            "test item 6440\n",
            "test item 6460\n",
            "test item 6480\n",
            "test item 6500\n",
            "test item 6520\n",
            "test item 6540\n",
            "test item 6560\n",
            "test item 6580\n",
            "test item 6600\n",
            "test item 6620\n",
            "test item 6640\n",
            "test item 6660\n",
            "test item 6680\n",
            "test item 6700\n",
            "test item 6720\n",
            "test item 6740\n",
            "test item 6760\n",
            "test item 6780\n",
            "test item 6800\n",
            "test item 6820\n",
            "test item 6840\n",
            "test item 6860\n",
            "test item 6880\n",
            "test item 6900\n",
            "test item 6920\n",
            "test item 6940\n",
            "test item 6960\n",
            "test item 6980\n",
            "test item 7000\n",
            "test item 7020\n",
            "test item 7040\n",
            "test item 7060\n",
            "test item 7080\n",
            "test item 7100\n",
            "test item 7120\n",
            "test item 7140\n",
            "test item 7160\n",
            "test item 7180\n",
            "test item 7200\n",
            "test item 7220\n",
            "test item 7240\n",
            "test item 7260\n",
            "test item 7280\n",
            "test item 7300\n",
            "test item 7320\n",
            "test item 7340\n",
            "test item 7360\n",
            "test item 7380\n",
            "test item 7400\n",
            "test item 7420\n",
            "test item 7440\n",
            "test item 7460\n",
            "test item 7480\n",
            "test item 7500\n",
            "test item 7520\n",
            "test item 7540\n",
            "test item 7560\n",
            "test item 7580\n",
            "test item 7600\n",
            "test item 7620\n",
            "test item 7640\n",
            "test item 7660\n",
            "test item 7680\n",
            "test item 7700\n",
            "test item 7720\n",
            "test item 7740\n",
            "test item 7760\n",
            "test item 7780\n",
            "test item 7800\n",
            "test item 7820\n",
            "test item 7840\n",
            "test item 7860\n",
            "test item 7880\n",
            "test item 7900\n",
            "test item 7920\n",
            "test item 7940\n",
            "test item 7960\n",
            "test item 7980\n",
            "test item 8000\n",
            "test item 8020\n",
            "test item 8040\n",
            "test item 8060\n",
            "test item 8080\n",
            "test item 8100\n",
            "test item 8120\n",
            "test item 8140\n",
            "test item 8160\n",
            "test item 8180\n",
            "test item 8200\n",
            "test item 8220\n",
            "test item 8240\n",
            "test item 8260\n",
            "test item 8280\n",
            "test item 8300\n",
            "test item 8320\n",
            "test item 8340\n",
            "test item 8360\n",
            "test item 8380\n",
            "test item 8400\n",
            "test item 8420\n",
            "test item 8440\n",
            "test item 8460\n",
            "test item 8480\n",
            "test item 8500\n",
            "test item 8520\n",
            "test item 8540\n",
            "test item 8560\n",
            "test item 8580\n",
            "test item 8600\n",
            "test item 8620\n",
            "test item 8640\n",
            "test item 8660\n",
            "test item 8680\n",
            "test item 8700\n",
            "test item 8720\n",
            "test item 8740\n",
            "test item 8760\n",
            "test item 8780\n",
            "test item 8800\n",
            "test item 8820\n",
            "test item 8840\n",
            "test item 8860\n",
            "test item 8880\n",
            "test item 8900\n",
            "test item 8920\n",
            "test item 8940\n",
            "test item 8960\n",
            "test item 8980\n",
            "test item 9000\n",
            "test item 9020\n",
            "test item 9040\n",
            "test item 9060\n",
            "test item 9080\n",
            "test item 9100\n",
            "test item 9120\n",
            "test item 9140\n",
            "test item 9160\n",
            "test item 9180\n",
            "test item 9200\n",
            "test item 9220\n",
            "test item 9240\n",
            "test item 9260\n",
            "test item 9280\n",
            "test item 9300\n",
            "test item 9320\n",
            "test item 9340\n",
            "test item 9360\n",
            "test item 9380\n",
            "test item 9400\n",
            "test item 9420\n",
            "test item 9440\n",
            "test item 9460\n",
            "test item 9480\n",
            "test item 9500\n",
            "test item 9520\n",
            "test item 9540\n",
            "test item 9560\n",
            "test item 9580\n",
            "test item 9600\n",
            "test item 9620\n",
            "test item 9640\n",
            "test item 9660\n",
            "test item 9680\n",
            "test item 9700\n",
            "test item 9720\n",
            "test item 9740\n",
            "test item 9760\n",
            "test item 9780\n",
            "test item 9800\n",
            "test item 9820\n",
            "test item 9840\n",
            "test item 9860\n",
            "test item 9880\n",
            "test item 9900\n",
            "test item 9920\n",
            "test item 9940\n",
            "test item 9960\n",
            "test item 9980\n",
            "test item 10000\n",
            "test item 10020\n",
            "test item 10040\n",
            "test item 10060\n",
            "test item 10080\n",
            "test item 10100\n",
            "test item 10120\n",
            "test item 10140\n",
            "test item 10160\n",
            "test item 10180\n",
            "test item 10200\n",
            "test item 10220\n",
            "test item 10240\n",
            "test item 10260\n",
            "test item 10280\n",
            "test item 10300\n",
            "test item 10320\n",
            "test item 10340\n",
            "test item 10360\n",
            "test item 10380\n",
            "test item 10400\n",
            "test item 10420\n",
            "test item 10440\n",
            "test item 10460\n",
            "test item 10480\n",
            "test item 10500\n",
            "test item 10520\n",
            "test item 10540\n",
            "test item 10560\n",
            "test item 10580\n",
            "test item 10600\n",
            "test item 10620\n",
            "test item 10640\n",
            "test item 10660\n",
            "test item 10680\n",
            "test item 10700\n",
            "test item 10720\n",
            "test item 10740\n",
            "test item 10760\n",
            "test item 10780\n",
            "test item 10800\n",
            "test item 10820\n",
            "test item 10840\n",
            "test item 10860\n",
            "test item 10880\n",
            "test item 10900\n",
            "test item 10920\n",
            "test item 10940\n",
            "test item 10960\n",
            "test item 10980\n",
            "test item 11000\n",
            "test item 11020\n",
            "test item 11040\n",
            "test item 11060\n",
            "test item 11080\n",
            "test item 11100\n",
            "test item 11120\n",
            "test item 11140\n",
            "test item 11160\n",
            "test item 11180\n",
            "test item 11200\n",
            "test item 11220\n",
            "test item 11240\n",
            "test item 11260\n",
            "test item 11280\n",
            "test item 11300\n",
            "test item 11320\n",
            "test item 11340\n",
            "test item 11360\n",
            "test item 11380\n",
            "test item 11400\n",
            "test item 11420\n",
            "test item 11440\n",
            "test item 11460\n",
            "test item 11480\n",
            "test item 11500\n",
            "test item 11520\n",
            "test item 11540\n",
            "test item 11560\n",
            "test item 11580\n",
            "test item 11600\n",
            "test item 11620\n",
            "test item 11640\n",
            "test item 11660\n",
            "test item 11680\n",
            "test item 11700\n",
            "test item 11720\n",
            "test item 11740\n",
            "test item 11760\n",
            "test item 11780\n",
            "test item 11800\n",
            "test item 11820\n",
            "test item 11840\n",
            "test item 11860\n",
            "test item 11880\n",
            "test item 11900\n",
            "test item 11920\n",
            "test item 11940\n",
            "test item 11960\n",
            "test item 11980\n",
            "test item 12000\n",
            "test item 12020\n",
            "test item 12040\n",
            "test item 12060\n",
            "test item 12080\n",
            "test item 12100\n",
            "test item 12120\n",
            "test item 12140\n",
            "test item 12160\n",
            "test item 12180\n",
            "test item 12200\n",
            "test item 12220\n",
            "test item 12240\n",
            "test item 12260\n",
            "test item 12280\n",
            "test item 12300\n",
            "test item 12320\n",
            "test item 12340\n",
            "test item 12360\n",
            "test item 12380\n",
            "test item 12400\n",
            "test item 12420\n",
            "test item 12440\n",
            "test item 12460\n",
            "test item 12480\n",
            "test item 12500\n",
            "test item 12520\n",
            "test item 12540\n",
            "test item 12560\n",
            "test item 12580\n",
            "test item 12600\n",
            "test item 12620\n",
            "test item 12640\n",
            "test item 12660\n",
            "test item 12680\n",
            "test item 12700\n",
            "test item 12720\n",
            "test item 12740\n",
            "test item 12760\n",
            "test item 12780\n",
            "test item 12800\n",
            "test item 12820\n",
            "test item 12840\n",
            "test item 12860\n",
            "test item 12880\n",
            "test item 12900\n",
            "test item 12920\n",
            "test item 12940\n",
            "test item 12960\n",
            "test item 12980\n",
            "test item 13000\n",
            "test item 13020\n",
            "test item 13040\n",
            "test item 13060\n",
            "test item 13080\n",
            "test item 13100\n",
            "test item 13120\n",
            "test item 13140\n",
            "test item 13160\n",
            "test item 13180\n",
            "test item 13200\n",
            "test item 13220\n",
            "test item 13240\n",
            "test item 13260\n",
            "test item 13280\n",
            "test item 13300\n",
            "test item 13320\n",
            "test item 13340\n",
            "test item 13360\n",
            "test item 13380\n",
            "test item 13400\n",
            "test item 13420\n",
            "test item 13440\n",
            "test item 13460\n",
            "test item 13480\n",
            "test item 13500\n",
            "test item 13520\n",
            "test item 13540\n",
            "test item 13560\n",
            "test item 13580\n",
            "test item 13600\n",
            "test item 13620\n",
            "test item 13640\n",
            "test item 13660\n",
            "test item 13680\n",
            "test item 13700\n",
            "test item 13720\n",
            "test item 13740\n",
            "test item 13760\n",
            "test item 13780\n",
            "test item 13800\n",
            "test item 13820\n",
            "test item 13840\n",
            "test item 13860\n",
            "test item 13880\n",
            "test item 13900\n",
            "test item 13920\n",
            "test item 13940\n",
            "test item 13960\n",
            "test item 13980\n",
            "test item 14000\n",
            "test item 14020\n",
            "test item 14040\n",
            "test item 14060\n",
            "test item 14080\n",
            "test item 14100\n",
            "test item 14120\n",
            "test item 14140\n",
            "test item 14160\n",
            "test item 14180\n",
            "test item 14200\n",
            "test item 14220\n",
            "test item 14240\n",
            "test item 14260\n",
            "test item 14280\n",
            "test item 14300\n",
            "test item 14320\n",
            "test item 14340\n",
            "test item 14360\n",
            "test item 14380\n",
            "test item 14400\n",
            "test item 14420\n",
            "test item 14440\n",
            "test item 14460\n",
            "test item 14480\n",
            "test item 14500\n",
            "test item 14520\n",
            "test item 14540\n",
            "test item 14560\n",
            "test item 14580\n",
            "test item 14600\n",
            "test item 14620\n",
            "test item 14640\n",
            "test item 14660\n",
            "test item 14680\n",
            "test item 14700\n",
            "test item 14720\n",
            "test item 14740\n",
            "test item 14760\n",
            "test item 14780\n",
            "test item 14800\n",
            "test item 14820\n",
            "test item 14840\n",
            "test item 14860\n",
            "test item 14880\n",
            "test item 14900\n",
            "test item 14920\n",
            "test item 14940\n",
            "test item 14960\n",
            "test item 14980\n",
            "test item 15000\n",
            "test item 15020\n",
            "test item 15040\n",
            "test item 15060\n",
            "test item 15080\n",
            "test item 15100\n",
            "test item 15120\n",
            "test item 15140\n",
            "test item 15160\n",
            "test item 15180\n",
            "test item 15200\n",
            "test item 15220\n",
            "test item 15240\n",
            "test item 15260\n",
            "test item 15280\n",
            "test item 15300\n",
            "test item 15320\n",
            "test item 15340\n",
            "test item 15360\n",
            "test item 15380\n",
            "test item 15400\n",
            "test item 15420\n",
            "test item 15440\n",
            "test item 15460\n",
            "test item 15480\n",
            "test item 15500\n",
            "test item 15520\n",
            "test item 15540\n",
            "test item 15560\n",
            "test item 15580\n",
            "test item 15600\n",
            "test item 15620\n",
            "test item 15640\n",
            "test item 15660\n",
            "test item 15680\n",
            "test item 15700\n",
            "test item 15720\n",
            "test item 15740\n",
            "test item 15760\n",
            "test item 15780\n",
            "test item 15800\n",
            "test item 15820\n",
            "test item 15840\n",
            "test item 15860\n",
            "test item 15880\n",
            "test item 15900\n",
            "test item 15920\n",
            "test item 15940\n",
            "test item 15960\n",
            "test item 15980\n",
            "test item 16000\n",
            "test item 16020\n",
            "test item 16040\n",
            "test item 16060\n",
            "test item 16080\n",
            "test item 16100\n",
            "test item 16120\n",
            "test item 16140\n",
            "test item 16160\n",
            "test item 16180\n",
            "test item 16200\n",
            "test item 16220\n",
            "test item 16240\n",
            "test item 16260\n",
            "test item 16280\n",
            "test item 16300\n",
            "test item 16320\n",
            "test item 16340\n",
            "test item 16360\n",
            "test item 16380\n",
            "test item 16400\n",
            "test item 16420\n",
            "test item 16440\n",
            "test item 16460\n",
            "test item 16480\n",
            "test item 16500\n",
            "test item 16520\n",
            "test item 16540\n",
            "test item 16560\n",
            "test item 16580\n",
            "test item 16600\n",
            "test item 16620\n",
            "test item 16640\n",
            "test item 16660\n",
            "test item 16680\n",
            "test item 16700\n",
            "test item 16720\n",
            "test item 16740\n",
            "test item 16760\n",
            "test item 16780\n",
            "test item 16800\n",
            "test item 16820\n",
            "test item 16840\n",
            "test item 16860\n",
            "test item 16880\n",
            "test item 16900\n",
            "test item 16920\n",
            "test item 16940\n",
            "test item 16960\n",
            "test item 16980\n",
            "test item 17000\n",
            "test item 17020\n",
            "test item 17040\n",
            "test item 17060\n",
            "test item 17080\n",
            "test item 17100\n",
            "test item 17120\n",
            "test item 17140\n",
            "test item 17160\n",
            "test item 17180\n",
            "test item 17200\n",
            "test item 17220\n",
            "test item 17240\n",
            "test item 17260\n",
            "test item 17280\n",
            "test item 17300\n",
            "test item 17320\n",
            "test item 17340\n",
            "test item 17360\n",
            "test item 17380\n",
            "test item 17400\n",
            "test item 17420\n",
            "test item 17440\n",
            "test item 17460\n",
            "test item 17480\n",
            "test item 17500\n",
            "test item 17520\n",
            "test item 17540\n",
            "test item 17560\n",
            "test item 17580\n",
            "test item 17600\n",
            "test item 17620\n",
            "test item 17640\n",
            "test item 17660\n",
            "test item 17680\n",
            "test item 17700\n",
            "test item 17720\n",
            "test item 17740\n",
            "test item 17760\n",
            "test item 17780\n",
            "test item 17800\n",
            "test item 17820\n",
            "test item 17840\n",
            "test item 17860\n",
            "test item 17880\n",
            "test item 17900\n",
            "test item 17920\n",
            "test item 17940\n",
            "test item 17960\n",
            "test item 17980\n",
            "test item 18000\n",
            "test item 18020\n",
            "test item 18040\n",
            "test item 18060\n",
            "test item 18080\n",
            "test item 18100\n",
            "test item 18120\n",
            "test item 18140\n",
            "test item 18160\n",
            "test item 18180\n",
            "test item 18200\n",
            "test item 18220\n",
            "test item 18240\n",
            "test item 18260\n",
            "test item 18280\n",
            "test item 18300\n",
            "test item 18320\n",
            "test item 18340\n",
            "test item 18360\n",
            "test item 18380\n",
            "test item 18400\n",
            "test item 18420\n",
            "test item 18440\n",
            "test item 18460\n",
            "test item 18480\n",
            "test item 18500\n",
            "test item 18520\n",
            "test item 18540\n",
            "test item 18560\n",
            "test item 18580\n",
            "test item 18600\n",
            "test item 18620\n",
            "test item 18640\n",
            "test item 18660\n",
            "test item 18680\n",
            "test item 18700\n",
            "test item 18720\n",
            "test item 18740\n",
            "test item 18760\n",
            "test item 18780\n",
            "test item 18800\n",
            "test item 18820\n",
            "test item 18840\n",
            "test item 18860\n",
            "test item 18880\n",
            "test item 18900\n",
            "test item 18920\n",
            "test item 18940\n",
            "test item 18960\n",
            "test item 18980\n",
            "test item 19000\n",
            "test item 19020\n",
            "test item 19040\n",
            "test item 19060\n",
            "test item 19080\n",
            "test item 19100\n",
            "test item 19120\n",
            "test item 19140\n",
            "test item 19160\n",
            "test item 19180\n",
            "test item 19200\n",
            "test item 19220\n",
            "test item 19240\n",
            "test item 19260\n",
            "test item 19280\n",
            "test item 19300\n",
            "test item 19320\n",
            "test item 19340\n",
            "test item 19360\n",
            "test item 19380\n",
            "test item 19400\n",
            "test item 19420\n",
            "test item 19440\n",
            "test item 19460\n",
            "test item 19480\n",
            "test item 19500\n",
            "test item 19520\n",
            "test item 19540\n",
            "test item 19560\n",
            "test item 19580\n",
            "test item 19600\n",
            "test item 19620\n",
            "test item 19640\n",
            "test item 19660\n",
            "test item 19680\n",
            "test item 19700\n",
            "test item 19720\n",
            "test item 19740\n",
            "test item 19760\n",
            "test item 19780\n",
            "test item 19800\n",
            "test item 19820\n",
            "test item 19840\n",
            "test item 19860\n",
            "test item 19880\n",
            "test item 19900\n",
            "test item 19920\n",
            "test item 19940\n",
            "test item 19960\n",
            "test item 19980\n",
            "test_set_with_ids 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0UIUgcBW6Ks",
        "outputId": "e9d4dd50-e36e-497d-d9c0-e7ee669a537d"
      },
      "source": [
        "sent=\"مش متأكد بصراحة\"\n",
        "test_tensor=feature_extraction_full(sent)\n",
        "test_tensor_numpy=test_tensor.detach().numpy()\n",
        "print(test_tensor.shape)\n",
        "print(test_tensor_numpy.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([9, 768])\n",
            "(9, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mOwZbfzEX8c",
        "outputId": "a911a7c4-79a8-4d98-bf0c-8f7591d7fcfb"
      },
      "source": [
        "#Step 5 - start training the RNN/LSTM\n",
        "import os\n",
        "from code_utils.align_utils import *\n",
        "from code_utils.arabic_lib import *\n",
        "from code_utils.general import * \n",
        "from itertools import groupby\n",
        "from collections import Counter\n",
        "import h5py\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "outcomes=[-1,0,1]\n",
        "\n",
        "exp_name=\"try_64_2layers_full_new\"\n",
        "models_dir=\"models\"\n",
        "exp_dir=os.path.join(models_dir,exp_name)\n",
        "if not os.path.exists(exp_dir): os.makedirs(exp_dir)\n",
        "\n",
        "n_input=768\n",
        "n_output=3\n",
        "n_hidden=64\n",
        "n_layers=2\n",
        "io_matching=False\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,io_matching)\n",
        "rnn.to(device)\n",
        "print(rnn)\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "LR=0.00001\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "\n",
        "\n",
        "n_epochs=50\n",
        "\n",
        "\n",
        "hdf5_fpath=\"data.hdf5\"\n",
        "hdf5_file=h5py.File(hdf5_fpath, 'r')\n",
        "\n",
        "train_grp=hdf5_file[\"train\"]\n",
        "dev_keys=list(train_grp.keys())[:2000]\n",
        "train_keys=list(train_grp.keys())[2000:]\n",
        "test_grp=hdf5_file[\"test\"]\n",
        "test_keys=list(test_grp.keys())\n",
        "\n",
        "for ep0 in range(n_epochs):\n",
        "  print(\"epoch #\",ep0)\n",
        "  model_fname=\"%s-%s.model\"%(exp_name,ep0)\n",
        "  PATH=os.path.join(exp_dir,model_fname)\n",
        "  if os.path.exists(PATH):\n",
        "    checkpoint = torch.load(PATH)\n",
        "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    rnn.train()\n",
        "    print(\"model found, proceeding to next epoch\")\n",
        "    continue\n",
        "    #rnn_model.to(device)    \n",
        "\n",
        "\n",
        "  train_loss=0\n",
        "  dev_loss=0\n",
        "  train_counter=0\n",
        "  dev_counter=0  \n",
        "  dev_n_correct=0\n",
        "  dev_preds_list=[]\n",
        "  #for sent_str,sentiment in train_set[:1000]:\n",
        "  for t_key in train_keys:\n",
        "    if train_counter%100==0: print(train_counter)\n",
        "    cur_hd_item=train_grp[t_key]\n",
        "    sentiment=cur_hd_item.attrs[\"sentiment\"]\n",
        "    text=cur_hd_item.attrs[\"text\"]\n",
        "    cur_feature_tensor=torch.tensor(cur_hd_item)\n",
        "    cur_feature_tensor=cur_feature_tensor.to(device)\n",
        "\n",
        "    # print(cur_hd_item.attrs[\"sentiment\"])\n",
        "    # print(cur_hd_item.attrs[\"text\"])\n",
        "    # print(torch.tensor(cur_hd_item))\n",
        "    # #print(dir(cur_hd_item))\n",
        "    # continue\n",
        "    \n",
        "    # #print(sent_str,sentiment)\n",
        "    # cur_feature_tensor=feature_extraction_full(sent_str)\n",
        "    correct_output_tensor=outcome2tensor(sentiment,outcomes)\n",
        "    correct_output_tensor=correct_output_tensor.to(device)\n",
        "    rnn.hidden = rnn.init_hidden()\n",
        "    rnn.zero_grad()\n",
        "    rnn_output=rnn(cur_feature_tensor)\n",
        "    rnn_output=rnn_output.to(device)\n",
        "    #print(rnn_output.shape)\n",
        "    #print(\"RNN output:\", rnn_output.view([n_output]))\n",
        "    #print(\"Correct output:\",correct_output_tensor)\n",
        "    correct_output_tensor=correct_output_tensor.view(rnn_output.shape)\n",
        "    loss = loss_func(rnn_output, correct_output_tensor)\n",
        "    #print(\"current loss:\", loss.item())\n",
        "    train_loss+=loss.item()\n",
        "    train_counter+=1\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"now working on dev_set\")\n",
        "  #for sent_str,sentiment in dev_set[:100]:\n",
        "  for dev_key in dev_keys:\n",
        "    if dev_counter%100==0: print(dev_counter)\n",
        "    cur_hd_item=train_grp[dev_key]\n",
        "    sentiment=cur_hd_item.attrs[\"sentiment\"]\n",
        "    text=cur_hd_item.attrs[\"text\"]\n",
        "    cur_feature_tensor=torch.tensor(cur_hd_item)\n",
        "    cur_feature_tensor=cur_feature_tensor.to(device)\n",
        "\n",
        "    #print(sent_str,sentiment)\n",
        "    #cur_feature_tensor=feature_extraction_full(sent_str)\n",
        "    correct_output_tensor=outcome2tensor(sentiment,outcomes)\n",
        "    correct_output_tensor=correct_output_tensor.to(device)\n",
        "    rnn.hidden = rnn.init_hidden()\n",
        "    rnn.zero_grad()\n",
        "    rnn_output=rnn(cur_feature_tensor)\n",
        "    rnn_output=rnn_output.to(device)\n",
        "    correct_output_tensor=correct_output_tensor.view(rnn_output.shape)\n",
        "    loss = loss_func(rnn_output, correct_output_tensor)\n",
        "    dev_loss+=loss.item()\n",
        "    dev_counter+=1\n",
        "    cur_pred_list=pred2label(rnn_output,outcomes)\n",
        "    top_pred=cur_pred_list[0][0]\n",
        "    is_correct=0\n",
        "    if top_pred==sentiment: \n",
        "      dev_n_correct+=1\n",
        "      is_correct=1\n",
        "    dev_preds_list.append((sentiment,is_correct))\n",
        "    #print(\"predicted:\", cur_pred_list, \"correct: \",sentiment)\n",
        "\n",
        "  #continue\n",
        "  print(\"total training loss:\",train_loss,\"count:\",train_counter,\"average loss:\", train_loss/train_counter)\n",
        "  avg_train_loss=train_loss/train_counter\n",
        "  print(\"total dev loss:\",dev_loss,\"count:\",dev_counter,\"average loss:\", dev_loss/dev_counter)\n",
        "  avg_dev_loss=dev_loss/dev_counter\n",
        "  dev_accuracy=round(dev_n_correct/dev_counter,4)\n",
        "  cur_pred_dist=pred_correctness_dist(dev_preds_list)\n",
        "  print(cur_pred_dist)\n",
        "  # print(Counter(all_preds_list))\n",
        "  # all_preds_list.sort()\n",
        "  print(\"n_correct:\",dev_n_correct, \"prediction accuracy:\", round(dev_n_correct/dev_counter,4))\n",
        "  print(\"---------\")\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': ep0,\n",
        "            'model_state_dict': rnn.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'dev_loss': avg_dev_loss,\n",
        "            'dev_accuracy': dev_accuracy, \n",
        "            'pred_dist': cur_pred_dist,\n",
        "            'n_input': n_input,\n",
        "            'n_output': n_output,\n",
        "            'n_hidden': n_hidden,\n",
        "            'n_layers': n_layers, \n",
        "            'outcomes': outcomes,\n",
        "            'io_matching':io_matching,\n",
        "            'LR':LR\n",
        "\n",
        "            }, PATH)  \n",
        "  #outcome_1hot=tensor.torch([])\n",
        "# print(\"now working on the test set\")\n",
        "# prediction_list=[]\n",
        "# test_i=0\n",
        "# #for tweet_id,sent_str in test_set_with_ids:\n",
        "# for test_key0 in test_keys:\n",
        "#   if test_i%100==0: print(test_i)\n",
        "#   cur_hd_item=test_grp[test_key0]\n",
        "#   #sentiment=cur_hd_item.attrs[\"sentiment\"]\n",
        "#   text=cur_hd_item.attrs[\"text\"]\n",
        "#   cur_feature_tensor=torch.tensor(cur_hd_item)  \n",
        "#   # continue\n",
        "  \n",
        "#   # #print(sent_str,sentiment)\n",
        "#   # cur_feature_tensor=feature_extraction_full(sent_str)\n",
        "#   rnn.hidden = rnn.init_hidden()\n",
        "#   rnn.zero_grad()\n",
        "#   rnn_output=rnn(cur_feature_tensor)\n",
        "#   cur_pred_list=pred2label(rnn_output,outcomes)\n",
        "#   top_pred=cur_pred_list[0][0]\n",
        "#   prediction_list.append((tweet_id,top_pred))\n",
        "#   test_i+=1\n",
        "#   # print(tweet_id,top_pred,sent_str)\n",
        "#   # print(\"-------\")\n",
        "\n",
        "# import csv\n",
        "# cur_fname=\"submission4.csv\"  \n",
        "# # field names \n",
        "# fields = ['Tweet_id', 'sentiment']    \n",
        "# # data rows of csv file \n",
        "# rows= prediction_list\n",
        "# with open(cur_fname, 'w') as f:\n",
        "#     # using csv.writer method from CSV package\n",
        "#     write = csv.writer(f)\n",
        "#     write.writerow(fields)\n",
        "#     write.writerows(rows)\n",
        "hdf5_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (lstm): LSTM(768, 64, num_layers=2)\n",
            "  (hidden2out): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n",
            "epoch # 0\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "now working on dev_set\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "total training loss: 4042.9237521421774 count: 31209 average loss: 0.12954352116832252\n",
            "total dev loss: 424.21789956899374 count: 2000 average loss: 0.21210894978449688\n",
            "[(-1, Counter({0: 361})), (0, Counter({1: 974, 0: 20})), (1, Counter({0: 524, 1: 121}))]\n",
            "n_correct: 1095 prediction accuracy: 0.5475\n",
            "---------\n",
            "epoch # 1\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "now working on dev_set\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "total training loss: 3688.3271900027453 count: 31209 average loss: 0.11818152423989059\n",
            "total dev loss: 393.6441270641917 count: 2000 average loss: 0.19682206353209586\n",
            "[(-1, Counter({0: 359, 1: 2})), (0, Counter({1: 971, 0: 23})), (1, Counter({0: 468, 1: 177}))]\n",
            "n_correct: 1150 prediction accuracy: 0.575\n",
            "---------\n",
            "epoch # 2\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "now working on dev_set\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "total training loss: 3548.044418371235 count: 31209 average loss: 0.11368657817844965\n",
            "total dev loss: 379.75442867921083 count: 2000 average loss: 0.18987721433960542\n",
            "[(-1, Counter({0: 352, 1: 9})), (0, Counter({1: 963, 0: 31})), (1, Counter({0: 450, 1: 195}))]\n",
            "n_correct: 1167 prediction accuracy: 0.5835\n",
            "---------\n",
            "epoch # 3\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "now working on dev_set\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "total training loss: 3441.6376369664613 count: 31209 average loss: 0.11027708792228079\n",
            "total dev loss: 370.43932435944953 count: 2000 average loss: 0.18521966217972477\n",
            "[(-1, Counter({0: 345, 1: 16})), (0, Counter({1: 960, 0: 34})), (1, Counter({0: 438, 1: 207}))]\n",
            "n_correct: 1183 prediction accuracy: 0.5915\n",
            "---------\n",
            "epoch # 4\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "now working on dev_set\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "total training loss: 3349.7002578222427 count: 31209 average loss: 0.10733122681989947\n",
            "total dev loss: 363.4148552976412 count: 2000 average loss: 0.1817074276488206\n",
            "[(-1, Counter({0: 325, 1: 36})), (0, Counter({1: 949, 0: 45})), (1, Counter({0: 424, 1: 221}))]\n",
            "n_correct: 1206 prediction accuracy: 0.603\n",
            "---------\n",
            "epoch # 5\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n",
            "30000\n",
            "30100\n",
            "30200\n",
            "30300\n",
            "30400\n",
            "30500\n",
            "30600\n",
            "30700\n",
            "30800\n",
            "30900\n",
            "31000\n",
            "31100\n",
            "31200\n",
            "now working on dev_set\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "total training loss: 3266.255005994751 count: 31209 average loss: 0.10465747079351313\n",
            "total dev loss: 358.8520422591255 count: 2000 average loss: 0.17942602112956274\n",
            "[(-1, Counter({0: 317, 1: 44})), (0, Counter({1: 936, 0: 58})), (1, Counter({0: 419, 1: 226}))]\n",
            "n_correct: 1206 prediction accuracy: 0.603\n",
            "---------\n",
            "epoch # 6\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HIWCBK7Yeww",
        "outputId": "3ec45ec0-c349-4642-bcc1-3f9af7db724d"
      },
      "source": [
        "#Step 6 - Now predicting using our best models\n",
        "import os, h5py\n",
        "import pandas as pd\n",
        "\n",
        "outcomes=[-1,0,1]\n",
        "\n",
        "exp_name=\"try_64_3_full_new\"\n",
        "models_dir=\"models\"\n",
        "exp_dir=os.path.join(models_dir,exp_name)\n",
        "if not os.path.exists(exp_dir): os.makedirs(exp_dir)\n",
        "epoch=18\n",
        "model_fname=\"%s-%s.model\"%(exp_name,epoch)\n",
        "PATH=os.path.join(exp_dir,model_fname)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "n_input=checkpoint['n_input']\n",
        "n_output=checkpoint['n_output']\n",
        "n_hidden=checkpoint['n_hidden']\n",
        "n_layers=checkpoint['n_layers']\n",
        "io_matching=False\n",
        "rnn_model = RNN(n_input, n_hidden, n_output,n_layers,io_matching)\n",
        "rnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "rnn_model.to(device)\n",
        "#rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "print(rnn_model)\n",
        "# epoch = checkpoint['epoch']\n",
        "# loss = checkpoint['loss']\n",
        "\n",
        "# n_input=768\n",
        "# n_output=3\n",
        "# n_hidden=64\n",
        "# n_layers=3\n",
        "# io_matching=False\n",
        "# rnn = RNN(n_input, n_hidden, n_output,n_layers,io_matching)\n",
        "\n",
        "\n",
        "# print(checkpoint.keys())\n",
        "for key,val in checkpoint.items():\n",
        "  if \"dict\" in key: continue\n",
        "  print(key,val)\n",
        "#rnn_model.hidden = rnn_model.init_hidden()\n",
        "#rnn_model.zero_grad()\n",
        "rnn_model.eval()\n",
        "\n",
        "# test_tensor=torch.rand([12, 768]).to(device)\n",
        "# raw_sent=\"أنا مش متأكد إيه اللي هيحصل هنا\"\n",
        "# sent_str=raw_sent.replace(\"<br>\",\" \")\n",
        "# sent_str=clean_ar(sent_str)\n",
        "# sent_toks=tok(sent_str)\n",
        "# sent_toks_str=\" \".join(sent_toks)\n",
        "# feature_tensor=sent2bert(sent_toks_str)\n",
        "\n",
        "#feature_tensor=feature_extraction_full(test_str).to(device)\n",
        "#out=model(feature_tensor)\n",
        "#out=model(test_tensor)\n",
        "# def predict(sent_str):\n",
        "#   feature_tensor=feature_extraction_full(sent_str).to(device)\n",
        "#   rnn_out=rnn_model(feature_tensor)\n",
        "#   print(rnn_out)\n",
        "\n",
        "# test=\"أنا مش متأكد إيه اللي هيحصل هنا\"\n",
        "# predict(test)\n",
        "# test=\"إيه الزفت دة\"\n",
        "# predict(test)\n",
        "\n",
        "hdf5_fpath=\"data.hdf5\"\n",
        "hdf5_file=h5py.File(hdf5_fpath, 'r')\n",
        "test_grp=hdf5_file[\"test\"]\n",
        "test_keys=list(test_grp.keys())\n",
        "test_df = pd.read_csv(\"test1_with_text.csv\")\n",
        "prediction_list=[]\n",
        "for i,a in enumerate(test_df.iterrows()):\n",
        "  rnn_model.init_hidden()\n",
        "  rnn_model.zero_grad()\n",
        "  #if i>50: break\n",
        "  if i%100==0: print(\"test item\", i)\n",
        "  row_dict=dict(iter(a[1].items())) \n",
        "  #print(row_dict)\n",
        "  tweet_id=str(row_dict[\"Tweet_id\"])\n",
        "  #print(tweet_id)\n",
        "  tweet_text=row_dict[\"Text\"]\n",
        "  cur_hd_item=test_grp[tweet_id]\n",
        "  #print(cur_hd_item.attrs[\"text\"])\n",
        "  cur_feature_tensor=torch.tensor(cur_hd_item).to(device)\n",
        "  #print(tweet_text)\n",
        "  #cur_feature_tensor=feature_extraction_full(tweet_text).to(device)\n",
        "  rnn_out=rnn_model(cur_feature_tensor)\n",
        "  cur_pred_list=pred2label(rnn_out,outcomes)\n",
        "  top_pred=cur_pred_list[0][0]\n",
        "\n",
        "  # if i<1000:\n",
        "  #   cur_feature_tensor=feature_extraction_full(tweet_text).to(device)\n",
        "  #   rnn_out=rnn_model(cur_feature_tensor)\n",
        "  #   cur_pred_list=pred2label(rnn_out,outcomes)\n",
        "  #   top_pred=cur_pred_list[0][0]\n",
        "  # else:\n",
        "  #   if \"زفت\" in tweet_text: top_pred=-1\n",
        "  #   else: top_pred=0\n",
        "\n",
        "    \n",
        "  #print(cur_pred_list)\n",
        "  prediction_list.append((tweet_id,top_pred))\n",
        "  #test_i+=1\n",
        "\n",
        "\n",
        "\n",
        "# print(\"now working on the test set\")\n",
        "# prediction_list=[]\n",
        "# test_i=0\n",
        "# #for tweet_id,sent_str in test_set_with_ids:\n",
        "# for test_key0 in test_keys:\n",
        "#   if test_i%100==0: print(test_i)\n",
        "#   cur_hd_item=test_grp[test_key0]\n",
        "#   #sentiment=cur_hd_item.attrs[\"sentiment\"]\n",
        "#   text=cur_hd_item.attrs[\"text\"]\n",
        "#   cur_feature_tensor=torch.tensor(cur_hd_item)  \n",
        "#   # continue\n",
        "  \n",
        "#   # #print(sent_str,sentiment)\n",
        "#   # cur_feature_tensor=feature_extraction_full(sent_str)\n",
        "#   rnn.hidden = rnn.init_hidden()\n",
        "#   rnn.zero_grad()\n",
        "#   rnn_output=rnn(cur_feature_tensor)\n",
        "#   cur_pred_list=pred2label(rnn_output,outcomes)\n",
        "#   top_pred=cur_pred_list[0][0]\n",
        "#   prediction_list.append((tweet_id,top_pred))\n",
        "#   test_i+=1\n",
        "#   # print(tweet_id,top_pred,sent_str)\n",
        "#   # print(\"-------\")\n",
        "\n",
        "import csv\n",
        "cur_fname=\"submission8.csv\"  \n",
        "# field names \n",
        "fields = ['Tweet_id', 'sentiment']    \n",
        "# data rows of csv file \n",
        "rows= prediction_list\n",
        "with open(cur_fname, 'w') as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerow(fields)\n",
        "    write.writerows(rows)\n",
        "\n",
        "\n",
        "\n",
        "hdf5_file.close()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (lstm): LSTM(768, 64, num_layers=3)\n",
            "  (hidden2out): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n",
            "epoch 18\n",
            "train_loss 0.06731816923196385\n",
            "dev_loss 0.1894536556837859\n",
            "dev_accuracy 0.632\n",
            "pred_dist [(-1, Counter({0: 258, 1: 103})), (0, Counter({1: 885, 0: 109})), (1, Counter({0: 369, 1: 276}))]\n",
            "n_input 768\n",
            "n_output 3\n",
            "n_hidden 64\n",
            "n_layers 3\n",
            "outcomes [-1, 0, 1]\n",
            "io_matching False\n",
            "LR 1e-05\n",
            "test item 0\n",
            "test item 100\n",
            "test item 200\n",
            "test item 300\n",
            "test item 400\n",
            "test item 500\n",
            "test item 600\n",
            "test item 700\n",
            "test item 800\n",
            "test item 900\n",
            "test item 1000\n",
            "test item 1100\n",
            "test item 1200\n",
            "test item 1300\n",
            "test item 1400\n",
            "test item 1500\n",
            "test item 1600\n",
            "test item 1700\n",
            "test item 1800\n",
            "test item 1900\n",
            "test item 2000\n",
            "test item 2100\n",
            "test item 2200\n",
            "test item 2300\n",
            "test item 2400\n",
            "test item 2500\n",
            "test item 2600\n",
            "test item 2700\n",
            "test item 2800\n",
            "test item 2900\n",
            "test item 3000\n",
            "test item 3100\n",
            "test item 3200\n",
            "test item 3300\n",
            "test item 3400\n",
            "test item 3500\n",
            "test item 3600\n",
            "test item 3700\n",
            "test item 3800\n",
            "test item 3900\n",
            "test item 4000\n",
            "test item 4100\n",
            "test item 4200\n",
            "test item 4300\n",
            "test item 4400\n",
            "test item 4500\n",
            "test item 4600\n",
            "test item 4700\n",
            "test item 4800\n",
            "test item 4900\n",
            "test item 5000\n",
            "test item 5100\n",
            "test item 5200\n",
            "test item 5300\n",
            "test item 5400\n",
            "test item 5500\n",
            "test item 5600\n",
            "test item 5700\n",
            "test item 5800\n",
            "test item 5900\n",
            "test item 6000\n",
            "test item 6100\n",
            "test item 6200\n",
            "test item 6300\n",
            "test item 6400\n",
            "test item 6500\n",
            "test item 6600\n",
            "test item 6700\n",
            "test item 6800\n",
            "test item 6900\n",
            "test item 7000\n",
            "test item 7100\n",
            "test item 7200\n",
            "test item 7300\n",
            "test item 7400\n",
            "test item 7500\n",
            "test item 7600\n",
            "test item 7700\n",
            "test item 7800\n",
            "test item 7900\n",
            "test item 8000\n",
            "test item 8100\n",
            "test item 8200\n",
            "test item 8300\n",
            "test item 8400\n",
            "test item 8500\n",
            "test item 8600\n",
            "test item 8700\n",
            "test item 8800\n",
            "test item 8900\n",
            "test item 9000\n",
            "test item 9100\n",
            "test item 9200\n",
            "test item 9300\n",
            "test item 9400\n",
            "test item 9500\n",
            "test item 9600\n",
            "test item 9700\n",
            "test item 9800\n",
            "test item 9900\n",
            "test item 10000\n",
            "test item 10100\n",
            "test item 10200\n",
            "test item 10300\n",
            "test item 10400\n",
            "test item 10500\n",
            "test item 10600\n",
            "test item 10700\n",
            "test item 10800\n",
            "test item 10900\n",
            "test item 11000\n",
            "test item 11100\n",
            "test item 11200\n",
            "test item 11300\n",
            "test item 11400\n",
            "test item 11500\n",
            "test item 11600\n",
            "test item 11700\n",
            "test item 11800\n",
            "test item 11900\n",
            "test item 12000\n",
            "test item 12100\n",
            "test item 12200\n",
            "test item 12300\n",
            "test item 12400\n",
            "test item 12500\n",
            "test item 12600\n",
            "test item 12700\n",
            "test item 12800\n",
            "test item 12900\n",
            "test item 13000\n",
            "test item 13100\n",
            "test item 13200\n",
            "test item 13300\n",
            "test item 13400\n",
            "test item 13500\n",
            "test item 13600\n",
            "test item 13700\n",
            "test item 13800\n",
            "test item 13900\n",
            "test item 14000\n",
            "test item 14100\n",
            "test item 14200\n",
            "test item 14300\n",
            "test item 14400\n",
            "test item 14500\n",
            "test item 14600\n",
            "test item 14700\n",
            "test item 14800\n",
            "test item 14900\n",
            "test item 15000\n",
            "test item 15100\n",
            "test item 15200\n",
            "test item 15300\n",
            "test item 15400\n",
            "test item 15500\n",
            "test item 15600\n",
            "test item 15700\n",
            "test item 15800\n",
            "test item 15900\n",
            "test item 16000\n",
            "test item 16100\n",
            "test item 16200\n",
            "test item 16300\n",
            "test item 16400\n",
            "test item 16500\n",
            "test item 16600\n",
            "test item 16700\n",
            "test item 16800\n",
            "test item 16900\n",
            "test item 17000\n",
            "test item 17100\n",
            "test item 17200\n",
            "test item 17300\n",
            "test item 17400\n",
            "test item 17500\n",
            "test item 17600\n",
            "test item 17700\n",
            "test item 17800\n",
            "test item 17900\n",
            "test item 18000\n",
            "test item 18100\n",
            "test item 18200\n",
            "test item 18300\n",
            "test item 18400\n",
            "test item 18500\n",
            "test item 18600\n",
            "test item 18700\n",
            "test item 18800\n",
            "test item 18900\n",
            "test item 19000\n",
            "test item 19100\n",
            "test item 19200\n",
            "test item 19300\n",
            "test item 19400\n",
            "test item 19500\n",
            "test item 19600\n",
            "test item 19700\n",
            "test item 19800\n",
            "test item 19900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDg6X6b0uGZ4",
        "outputId": "bdd2e6a0-9296-4ab2-e4c2-4357fe4d5ef5"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "exp_name=\"try_64_3_full_new\"\n",
        "models_dir=\"models\"\n",
        "exp_dir=os.path.join(models_dir,exp_name)\n",
        "if not os.path.exists(exp_dir): os.makedirs(exp_dir)\n",
        "for epoch in range(50):\n",
        "  \n",
        "  model_fname=\"%s-%s.model\"%(exp_name,epoch)\n",
        "  PATH=os.path.join(exp_dir,model_fname)\n",
        "  if not os.path.exists(PATH): continue\n",
        "  print(epoch)\n",
        "  checkpoint = torch.load(PATH)\n",
        "  print(\"train_loss\", checkpoint[\"train_loss\"])\n",
        "  print(\"dev_loss\", checkpoint[\"dev_loss\"])\n",
        "  print(\"pred_dist\", checkpoint[\"pred_dist\"])\n",
        "  print(\"dev_accuracy\", checkpoint[\"dev_accuracy\"])\n",
        "  print(\"--------\")\n",
        " "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "train_loss 0.13290933960558338\n",
            "dev_loss 0.19821556433735169\n",
            "pred_dist [(-1, Counter({0: 358, 1: 3})), (0, Counter({1: 972, 0: 22})), (1, Counter({0: 510, 1: 135}))]\n",
            "dev_accuracy 0.555\n",
            "--------\n",
            "1\n",
            "train_loss 0.11855566232499826\n",
            "dev_loss 0.1852338074836007\n",
            "pred_dist [(-1, Counter({0: 355, 1: 6})), (0, Counter({1: 960, 0: 34})), (1, Counter({0: 406, 1: 239}))]\n",
            "dev_accuracy 0.6025\n",
            "--------\n",
            "2\n",
            "train_loss 0.11369803979148257\n",
            "dev_loss 0.1807393021238651\n",
            "pred_dist [(-1, Counter({0: 310, 1: 51})), (0, Counter({1: 930, 0: 64})), (1, Counter({0: 404, 1: 241}))]\n",
            "dev_accuracy 0.611\n",
            "--------\n",
            "3\n",
            "train_loss 0.10989065623901201\n",
            "dev_loss 0.1793657651357039\n",
            "pred_dist [(-1, Counter({0: 289, 1: 72})), (0, Counter({1: 921, 0: 73})), (1, Counter({0: 412, 1: 233}))]\n",
            "dev_accuracy 0.613\n",
            "--------\n",
            "4\n",
            "train_loss 0.10680578951682725\n",
            "dev_loss 0.17788268529832477\n",
            "pred_dist [(-1, Counter({0: 279, 1: 82})), (0, Counter({1: 912, 0: 82})), (1, Counter({0: 411, 1: 234}))]\n",
            "dev_accuracy 0.614\n",
            "--------\n",
            "5\n",
            "train_loss 0.10395111685933953\n",
            "dev_loss 0.17649559115526972\n",
            "pred_dist [(-1, Counter({0: 267, 1: 94})), (0, Counter({1: 900, 0: 94})), (1, Counter({0: 414, 1: 231}))]\n",
            "dev_accuracy 0.6125\n",
            "--------\n",
            "6\n",
            "train_loss 0.1012197525107083\n",
            "dev_loss 0.17776689896338804\n",
            "pred_dist [(-1, Counter({0: 275, 1: 86})), (0, Counter({1: 913, 0: 81})), (1, Counter({0: 405, 1: 240}))]\n",
            "dev_accuracy 0.6195\n",
            "--------\n",
            "7\n",
            "train_loss 0.09845639709005283\n",
            "dev_loss 0.17737904412076888\n",
            "pred_dist [(-1, Counter({0: 266, 1: 95})), (0, Counter({1: 909, 0: 85})), (1, Counter({0: 407, 1: 238}))]\n",
            "dev_accuracy 0.621\n",
            "--------\n",
            "8\n",
            "train_loss 0.09578771865331354\n",
            "dev_loss 0.17669970838479276\n",
            "pred_dist [(-1, Counter({0: 261, 1: 100})), (0, Counter({1: 898, 0: 96})), (1, Counter({0: 395, 1: 250}))]\n",
            "dev_accuracy 0.624\n",
            "--------\n",
            "9\n",
            "train_loss 0.09292335413695423\n",
            "dev_loss 0.17833428006934934\n",
            "pred_dist [(-1, Counter({0: 261, 1: 100})), (0, Counter({1: 902, 0: 92})), (1, Counter({0: 404, 1: 241}))]\n",
            "dev_accuracy 0.6215\n",
            "--------\n",
            "10\n",
            "train_loss 0.09005422395566806\n",
            "dev_loss 0.18045251606032628\n",
            "pred_dist [(-1, Counter({0: 259, 1: 102})), (0, Counter({1: 898, 0: 96})), (1, Counter({0: 398, 1: 247}))]\n",
            "dev_accuracy 0.6235\n",
            "--------\n",
            "11\n",
            "train_loss 0.08708932380365636\n",
            "dev_loss 0.1802741087907947\n",
            "pred_dist [(-1, Counter({0: 262, 1: 99})), (0, Counter({1: 892, 0: 102})), (1, Counter({0: 388, 1: 257}))]\n",
            "dev_accuracy 0.624\n",
            "--------\n",
            "12\n",
            "train_loss 0.08407967567197461\n",
            "dev_loss 0.18069412670736246\n",
            "pred_dist [(-1, Counter({0: 256, 1: 105})), (0, Counter({1: 882, 0: 112})), (1, Counter({0: 383, 1: 262}))]\n",
            "dev_accuracy 0.6245\n",
            "--------\n",
            "13\n",
            "train_loss 0.08098064061300252\n",
            "dev_loss 0.1847035052287597\n",
            "pred_dist [(-1, Counter({0: 264, 1: 97})), (0, Counter({1: 889, 0: 105})), (1, Counter({0: 382, 1: 263}))]\n",
            "dev_accuracy 0.6245\n",
            "--------\n",
            "14\n",
            "train_loss 0.07808921803858297\n",
            "dev_loss 0.18951623327729789\n",
            "pred_dist [(-1, Counter({0: 253, 1: 108})), (0, Counter({1: 891, 0: 103})), (1, Counter({0: 416, 1: 229}))]\n",
            "dev_accuracy 0.614\n",
            "--------\n",
            "15\n",
            "train_loss 0.07519460593234739\n",
            "dev_loss 0.184769876214967\n",
            "pred_dist [(-1, Counter({0: 261, 1: 100})), (0, Counter({1: 892, 0: 102})), (1, Counter({0: 379, 1: 266}))]\n",
            "dev_accuracy 0.629\n",
            "--------\n",
            "16\n",
            "train_loss 0.0724344390419316\n",
            "dev_loss 0.18672742987163723\n",
            "pred_dist [(-1, Counter({0: 232, 1: 129})), (0, Counter({1: 876, 0: 118})), (1, Counter({0: 389, 1: 256}))]\n",
            "dev_accuracy 0.6305\n",
            "--------\n",
            "17\n",
            "train_loss 0.06959428153650822\n",
            "dev_loss 0.19162592026252304\n",
            "pred_dist [(-1, Counter({0: 245, 1: 116})), (0, Counter({1: 876, 0: 118})), (1, Counter({0: 388, 1: 257}))]\n",
            "dev_accuracy 0.6245\n",
            "--------\n",
            "18\n",
            "train_loss 0.06731816923196385\n",
            "dev_loss 0.1894536556837859\n",
            "pred_dist [(-1, Counter({0: 258, 1: 103})), (0, Counter({1: 885, 0: 109})), (1, Counter({0: 369, 1: 276}))]\n",
            "dev_accuracy 0.632\n",
            "--------\n",
            "19\n",
            "train_loss 0.06450256683621838\n",
            "dev_loss 0.1909396380183748\n",
            "pred_dist [(-1, Counter({0: 238, 1: 123})), (0, Counter({1: 864, 0: 130})), (1, Counter({0: 384, 1: 261}))]\n",
            "dev_accuracy 0.624\n",
            "--------\n",
            "20\n",
            "train_loss 0.06218015498633387\n",
            "dev_loss 0.19196901932460816\n",
            "pred_dist [(-1, Counter({0: 223, 1: 138})), (0, Counter({1: 854, 0: 140})), (1, Counter({0: 382, 1: 263}))]\n",
            "dev_accuracy 0.6275\n",
            "--------\n",
            "21\n",
            "train_loss 0.06028705517423061\n",
            "dev_loss 0.2091259027548217\n",
            "pred_dist [(-1, Counter({0: 265, 1: 96})), (0, Counter({1: 907, 0: 87})), (1, Counter({0: 432, 1: 213}))]\n",
            "dev_accuracy 0.608\n",
            "--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWr0WDNIsoMA",
        "outputId": "3328d2dd-c900-4d0b-e2a8-0a7d4df1d175"
      },
      "source": [
        "def predict(sent_str):\n",
        "  feature_tensor=feature_extraction_full(sent_str)\n",
        "  rnn_out=rnn(feature_tensor)\n",
        "  return rnn_out\n",
        "  #print(rnn_out)\n",
        "\n",
        "test=\"أنا مش متأكد إيه اللي هيحصل هنا\"\n",
        "predict(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1649, 0.7413, 0.1047]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v58aODGaswj",
        "outputId": "7074be7a-b640-4651-e147-23c647cd6a54"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "#op=requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)\n",
        "train_df = pd.read_csv(\"training_file.csv\")\n",
        "limit=40000\n",
        "all_ids=[]\n",
        "for i,a in enumerate(train_df.iterrows()):\n",
        "  #print(a)\n",
        "  tw_id=str(a[1].Tweet_id)\n",
        "  sentiment=a[1].sentiment\n",
        "  all_ids.append(tw_id)\n",
        "\n",
        "  # print(\">>>\",tw_id, sentiment)\n",
        "  # if i>30: break\n",
        "  # url='https://twitter.com/i/web/status/'+tw_id\n",
        "  # print(url)\n",
        "  # op=requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)\n",
        "  # html_content=op.text\n",
        "print(len(all_ids))\n",
        "first_part=\"\\n\".join([str(v) for v in all_ids[:40000]])\n",
        "second_part=\"\\n\".join([str(v) for v in all_ids[40000:]])\n",
        "fopen=open(\"first.csv\",\"w\")\n",
        "fopen.write(first_part)\n",
        "fopen.close()\n",
        "fopen=open(\"second.csv\",\"w\")\n",
        "fopen.write(second_part)\n",
        "fopen.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO9UjNbbq1OM",
        "outputId": "937667fa-ffbc-489f-9ead-14393b965cab"
      },
      "source": [
        "#Run this code to combine the two parts of the WTI training data\n",
        "import pandas as pd\n",
        "import json\n",
        "train_df = pd.read_csv(\"training_file.csv\")\n",
        "all_ids=[]\n",
        "sentiment_dict={}\n",
        "for i,a in enumerate(train_df.iterrows()):\n",
        "  #print(a)\n",
        "  tw_id=str(a[1].Tweet_id)\n",
        "  sentiment=a[1].sentiment\n",
        "  sentiment_dict[tw_id]=sentiment\n",
        "  all_ids.append(tw_id)\n",
        "\n",
        "first_fopen=open(\"first.json\")\n",
        "first_content=first_fopen.read()\n",
        "first_fopen.close()\n",
        "first_list=json.loads(first_content)\n",
        "\n",
        "second_fopen=open(\"second.json\")\n",
        "second_content=second_fopen.read()\n",
        "second_fopen.close()\n",
        "second_list=json.loads(second_content)\n",
        "\n",
        "print(len(first_list), len(second_list))\n",
        "\n",
        "combined_fopen=open(\"combined_training.tsv\",\"w\")\n",
        "combined=first_list+second_list\n",
        "for a in combined:\n",
        "  cur_id=a[\"id\"]\n",
        "  cur_txt=a[\"text\"]\n",
        "  cur_txt=cur_txt.replace(\"\\n\",\"<br>\").replace(\"\\r\",\"<br>\").replace(\"\\t\",\" \")\n",
        "  cur_sentiment=sentiment_dict[cur_id]\n",
        "  line=\"%s\\t%s\\t%s\\n\"%(cur_id,cur_sentiment,cur_txt)\n",
        "  combined_fopen.write(line)\n",
        "  # print(cur_id,cur_txt,cur_sentiment)\n",
        "  # print(\"----\")\n",
        "combined_fopen.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24614 8595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2BotownjpBB",
        "outputId": "e4b2f8c6-cd13-4e8f-b7ee-c8aa32aa2761"
      },
      "source": [
        "for s in second_list[:100]:\n",
        "  print(s[\"text\"])\n",
        "  print(\"-----\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@SaudiHousingCC \n",
            "هل احتساب قيمة دعم القرض تكون\n",
            "الراتب الاساسي + بدل السكن مخصوم منه مستحقات التامينات ؟؟\n",
            "-----\n",
            "@SaudiHousing لم يتم ايداع الدعم السكني في حسابي هذا الشهر\n",
            "-----\n",
            "@ejar_sa السلام عليكم متى يتم شحن الرصيد بعد سداد الفاتوره بالبنك\n",
            "-----\n",
            "صراحة مرا مووقتكم  حجرصحي وشبكة زفت مرا نبغى حل جذري.    @STC\n",
            "-----\n",
            "المافيا في ايطاليا تبرعت للحكومه  الإيطالية 10 مليار دولار لمواجهة  فيروس كورونا ..فيكى الخير يا مافيا✋😂\n",
            "-----\n",
            "@ejar_sa  السلام عليكم حبيت اسال هل يلزم المستاجر بدفع مبلغ مالي من أجل اصدار العقد الإلكتروني\n",
            "-----\n",
            "@majedhogail الاكثريه مانزل لهم الدعم نتمنا توضيح السبب علما بأن الاشهر السابقه ينزل ولا في اي سبب علما بأنه وصلت رساله من معاليك\n",
            "-----\n",
            "المفروض من باب الادب ان\n",
            "@stc\n",
            "@Mobily\n",
            "@Zain\n",
            "يفتحو تطبيقات مكالمات الفيديو مجانا خلال فترة منع التجول على الاقل\n",
            "\n",
            "وضعية الميت قبيحه\n",
            "-----\n",
            "@SaudiHousing السلام عليكم\n",
            "هل يمكن الحصول على القرض الحسن الخاص بمنسوبي التعليم بدون القرض المدعوم\n",
            "-----\n",
            "#أمر_ملكي: تكليف ماجد الحقيل وزير الإسكان بالقيام بمهام وزير الشؤون البلدية والقروية بالإضافة إلى عمله كوزير للإسكان\n",
            "-----\n",
            "@SaudiHousingCC سلام عليكم وش شروط الدفعه المقدمة من سكني للمدنيين\n",
            "-----\n",
            "@majedhogail السلام عليكم \n",
            "انا موظف عسكري وابي بناء ذاتي وتم التقديم وتم رفضي بسبب العمر انه اقل من 25  وانا متزوج\n",
            "-----\n",
            "@ejar_sa مرحبا  هل لابد من تجديد العقد مع الوسيط العقاري  قبل تجديده  مع  المستأجر في موقع إيجار\n",
            "-----\n",
            "@SaudiHousingCC لدي ارض مرهونة للبنك هل يمكن الاستفادة من الدعم للبناء ؟\n",
            "-----\n",
            "السلام عليكم ورحمة الله وبركاتة ماذا بعد طلب اخلاء العقار\n",
            "ماهي الاليه المتبعه في اخلاء مستاجر شقه @ejar_sa\n",
            "-----\n",
            "حسبي الله عليك يا اسوء شركة اتصالات...\n",
            "تشحن رصيدك وتكتشف بعد يومين انهم سحبوا الرصيد كله وانت ما أجريت ولا اتصال\n",
            "\n",
            "@stc\n",
            "@stc_ksa\n",
            "-----\n",
            "@ejar_sa لا يكفي الاعتذار اريد حل سريع او ابلاغي باللغاء السجل وفتح سجل جديد\n",
            "-----\n",
            "@majedhogail اللهم ارزقني في بيت لي ولاولادي...العمر٥٢..اللهم امين يارب\n",
            "-----\n",
            "@MojKsa السلام عليكم فيه ارض مساحتها تفوق ٢٥٠٠متر كيف انقل ملكيتها ؟\n",
            "-----\n",
            "France2 عاملة حلقة توجيهية تثقفية عن Coronavirus . \n",
            "بلبنان وسائل الإعلام شو عملوا غير نقل الشتائم ونشر الخوف والتحريض والتهويل ؟🤔🤔🤔\n",
            "-----\n",
            "@ejar_sa \n",
            "\n",
            "عزيزي هل سيتم ايقاف الدورات القادمه نظرا للاوضاع الصحيه الطارئه والحرص على سلامه الجميع ؟\n",
            "-----\n",
            "قد يمكن ولكن لا هههههه\n",
            "كان الله في عونك يا ابو عزام اذا الموضوع عند شركة @stc ضاع حقك https://t.co/lH8C1HUXnT\n",
            "-----\n",
            "بحث عن مواصفات مرضى فيروس كورونا : الأعراض المرضية ، النتائج المعملية والأشعات .\n",
            "\n",
            "(دراسة وصفية ) https://t.co/Ttv0xxCzOI\n",
            "-----\n",
            "الوزير المفضل عندي معالي الاستاذ توفيق الربيعة ويجي بعده معالي الاستاذ ماجد الحقيل،، الحقيل قادم وبقوة ويارب تعينه\n",
            "-----\n",
            "@SaudiMOH تطعيم الاطفال كيف يتم ؟؟\n",
            "أين يذهب الطفل ؟ او هل مندوب منكم يزورنا في البيت ؟\n",
            "-----\n",
            "@SaudiHousingCC                             مذا تعني هذي الرسالة لوتكرمتم\n",
            "\n",
            "عزيزى علي محمد عبدالله الشهري تم تعليق الشهادة لغرض الاسترداد\n",
            "-----\n",
            "@SaudiHousingCC \n",
            "السلام عليكم ورحمه الله وبركاته\n",
            "استفسار ميف احجز في مشروع مكه المكرمه شقق نموذج (ج)\n",
            "-----\n",
            "@MCgovSA سعر كيلو الليمون اليوم 14ريال وسعر الكمامات الكرتون ابو خمسين حبة 50ريال !!؟؟؟\n",
            "-----\n",
            "@majedhogail يامعالي الوزير ارجو حل مشكلة نبض قيران والمواطنين المتضررين واخذها بمحمل الجد\n",
            "-----\n",
            "@SaudiMOH937 هل يمديني اطلب كمامات ومعقم من وزارة الصحة؟وهل يكون مجاناً او لا؟\n",
            "-----\n",
            "جزء كبير من راحتك النفسيه بيتلخص في إنك لا تعرف حاجه عن حد ، ولا حد يعرف عنك حاجه \"\n",
            "-----\n",
            "@MCgovSA السلام عليكم\n",
            "عندي استفسار عن امكانية تسجيل اكثر من سجل تجاري تحت رقم المنشاة نفسه من مكت العمل\n",
            "-----\n",
            "المرض النفسي والعقلي قاعد ينتشر أكثر من كورونا 😭😭\n",
            "-----\n",
            "@MoweSa السلام عليكم\n",
            "عندي بطاقة صحية في النظام وماعرفت اطبعها\n",
            "ممكن طريقة الطباعه أو رقمها\n",
            "-----\n",
            "ما فاتك لم يخلق لك، و ما خلق لك لن يفوتك\n",
            "-----\n",
            "سهرة مع شعر عمرو حسن ومحمد ابراهيم عظمهه ♥️♥️🔝\n",
            "-----\n",
            "أنا شخص يغرقّك بالفرص بس ما تدري متى تكون الأخيرة.\n",
            "-----\n",
            "@SaudiMOH السلام عليكم لوسمحت عندي موعد صرف علاج في مستشفى المدائن بجده ابغ اسال كيف طريقةصرف العلاج\n",
            "-----\n",
            "@MoweSa انا بروح من الرياض للقصيم ودخلت موقع التصريح ولا يوجد خيار القصيم مالعمل؟\n",
            "-----\n",
            "@MojKsa هل المحاكم تفتح ابوابها بيوم 6 رمضان وكيف طريقه اعاده الجلسة بتاريخ فايت بسبب الازمه لئن فاتني جلسه بوقت الازمه\n",
            "-----\n",
            "@SaudiHousingCC \n",
            "كيف يمكنني التراجع عن الغاء طلب العقد من وزراه الاسكان 🥺\n",
            "-----\n",
            "#متضرري_الصندوق_العقاري\n",
            "\n",
            "يَمْحَقُ اللَّهُ الرِّبَا وَيُرْبِي الصَّدَقَاتِ وَاللَّهُ لَا يُحِبُّ كُلَّ كَفَّارٍ أَثِيمٍ\n",
            "-----\n",
            "كورونا فيه احتمالية شفاء ، لكن مرضى العقيدة ما فيهم طب . https://t.co/lBaLKVzpuD\n",
            "-----\n",
            "@SaudiHousingCC السلام عليكم \n",
            "هل معارض سكني مستمرين في العمل الان ولا تم تعليقهم بسبب كورونا ؟\n",
            "-----\n",
            "@SaudiHousing السلام عليكم ورحمة الله وبركاته بما ان القطاعات الحكوميه بإجازه هل من موقع استطيع اخذ صك الارض من الوزاره..\n",
            "-----\n",
            "@AlAhliNCB السلام عليكم استفسار ليه اغلب المحطات والبقايل والمراكز ما يقبلو بطايق الصرافات ماستر كارد ،، ايش المشكلة\n",
            "-----\n",
            "@SaudiHousing استفسار \n",
            "\n",
            "انا ارفقت رخصه البناء قي موقع سكني لكن بعدل فيها \n",
            "\n",
            "كيف الطريقه عشان اعدل رخصه البناء وارفقها ثاني ؟\n",
            "-----\n",
            "كورونا جند من جنود الله،  الله عليك يا كرونا!! https://t.co/bHpwtmnz7h\n",
            "-----\n",
            "ماعندي مشكله مع الحجر كثر ما عندي مشكله مع @stc اسوء بيانات بالكون.. عيب والله\n",
            "-----\n",
            "@SaudiHousingCC السلام عليكم\n",
            "عندي فلل على شارع تجاري هل استطيع البيع على مستفيدي الاسكان والعقار تجاري بدون محلات فقط فيلا سكنية ؟\n",
            "-----\n",
            "اليوم علق النت علي وقطع وانا اختبر شكرًا حبايبي @stc 🤝.\n",
            "-----\n",
            "@stc @Mobily @ZainKSA من هي الشركة التي توفر لي أكبر سرعه منزلية لايهم السعر مع العلم يوجد عوازل في المنزل 🤓@CITC_SA\n",
            "-----\n",
            "@SaudiHousingCC  مريت سكني وكل المشاريع محجووزه بالكاامل ولافيه شي وش الحل\n",
            "-----\n",
            "@stc_kwt   @stc عفواً بس  محد تواصل وياي !!  بعرف شهرين وانا انطر https://t.co/cfkZDnD7SO\n",
            "-----\n",
            "#الشوارع_الحين\n",
            "ياأهل الطايف\n",
            "تلاحظون فيه ضعف بالنت بالطايف بالذات@stc\n",
            "@Mobily   elife\n",
            "-----\n",
            "#يا_خادم_الحرمين_الشريفين\n",
            " \n",
            "@ZainKSA \n",
            "@ZainHelpSA\n",
            "ولم يردون الله يخلي لي جارنا عندة @stc  وشابك معهم\n",
            "-----\n",
            "@SaudiHousing الله يسعدك اريد الحجز بمخطط ولي العهد ٩ بمكة وماعرفت ضروري قبل تنتهي لاسمح الله\n",
            "-----\n",
            "كورونا VS  هانتا\n",
            "الخفاش VS الفار\n",
            "ياختشششي كدا ينفع وكدا ينفع\n",
            "-----\n",
            "قسم بالله فوق وباء كورونا متسلطهههه علينا @stc عدلووو شبكتكممممممم مو طبيعي علشان١٣ ثانيه نجلس ننتظر١٣ دقيقه😡😡😡\n",
            "-----\n",
            "حتى انتوا عدلتوا سالفة تجديد ال ٥ جيجا و خليتوها مره وحده والا ليلحين چنكم ما تدرون ؟؟ ترى زين اسمعوا الكلام @stc\n",
            "-----\n",
            "@HajMinistry السلام عليكم ماذا عن المتواجدين أصحاب العمره ولم يسمح لهم بالخروج من المنفذ\n",
            "-----\n",
            "@Saudia_Care السلام عليكم ، اتواصل عن طريق الواتس ما في فائدة او رد علينا ، ارجو التواصل على الخاص لاجراء تعديل على الحجز\n",
            "-----\n",
            "هالايام تخاف حتى تعطس يخافون انه كورونا 💔.\n",
            "-----\n",
            "@SaudiHousingCC ماهو رمز المشروع لمدينة الورود بالطائف؟ مطلوب في المستشار العقاري\n",
            "-----\n",
            "@MojCare السلام عليكم عقد الايجار الموحد اذا كان منتهي هل اتقدم بدعوى للمحكمة او تظل له نفس القوة التنفيذيه\n",
            "-----\n",
            "وهل يجروء فيروس  كورونا أن يدخل اليمن ونحن نستند إلى قبيلة مثل حاشد .🤨🤫\n",
            "-----\n",
            "@stc اقسم بالله انتم اسوء شركة وبتبقون اسوء شركة طول عمركم\n",
            "-----\n",
            "@sakani_housing بنسبه مبادرة دعم المدنيين مخصصة فقط لشراء وحدة سكنية\n",
            "-----\n",
            "@stc الله يفكنا منكم انتم والنت حقكم اللي الواحد مايقدر يتابع فلم حتى بجودة ضعيفة\n",
            "-----\n",
            "@ejar_sa اخوي رقمي يجي عليه اني مستاجر وانا ماستاجرت وادخل برقم الهويه يقول غير مطابق للجوال وش السالفه\n",
            "-----\n",
            "@SaudiHousing \n",
            "السلام عليكم \n",
            "ممكن موقع مشروع الفلل في أبها \n",
            "احتاج الموقع ضروري \n",
            "\n",
            "وشكرا\n",
            "-----\n",
            "@SaudiHousingCC استفسار هل سيتم ايقاف دعم البناء الذاتي حسب تغريده سابقه لكم قبل عام انه الدعم الذاتي مستمر حتى شهر مارس 2020\n",
            "-----\n",
            "عشان كنا مؤدبين اليوم، إفتحولنا كم محل شاورما بكرا\n",
            "-----\n",
            "عاجل الجزيرة| وزير الصحة التونسي: 59 إصابة جديدة بفيروس كورونا وارتفاع عدد المصابين إلى 173\n",
            "-----\n",
            "من طرائف ٢٠٢٠\n",
            "أنّ موريتانيا\n",
            "تعانى من مرض \n",
            "غَبْرُونَا \n",
            "بدلا من \n",
            "كورونا \n",
            "🤗\n",
            "-----\n",
            "شركات الاتصالات ممكن تأجلون فواتير هل شهر للشهر الجاي @ZainKuwait @stc @OoredooKuwait\n",
            "-----\n",
            "@stc ممكن اعرف متى رح يتعدل الواي فاي ومتى تتعدل خداماتكم؟ لين متى يعني كل يوم تتلفون اعصابنا!!!\n",
            "-----\n",
            "@dev_housing السلام عليكم انا من المستفيدين للضمان الاجتماعي ومسجله بالاسكان وعندي مستحق الحاله كيف احصل على سكن\n",
            "-----\n",
            "@STC لو سمحتم وراي كلاسات احضرها :) فكونا من استلعان الشبكه\n",
            "-----\n",
            "الصحة: \n",
            "اصحاب فصيلة الدم A الاكثر تعرضاً للاصابة بفايروس #كورونا.\n",
            "-----\n",
            "على طاري القعده بالبيت ياليت شركات الاتصال تقوي الشبكات لانه مايصير محجور وانت ضعيف كيف نتابع الافلام والسنابات\n",
            "@stc\n",
            "@ZainKSA\n",
            "@Mobily\n",
            "-----\n",
            "@saudimomra @majedhogail نحتاج دعمكم وتوفير معلب يليق بشباب تلك المحافظه https://t.co/sBoTuBQors\n",
            "-----\n",
            "@SaudiHousingCC مبلغ الدعم السكني لم ينزل لي الشهر هذا\n",
            "-----\n",
            "@SaudiHousing وصلتني هذه الرسالة النصية ولم افهمها \n",
            "\n",
            "عزيزى سلطان الخ .. تم تعليق الشهادة لغرض الاسترداد\n",
            "\n",
            "مالمقصود بذلك ؟\n",
            "-----\n",
            "عاجل: #الصحة_الأميركية: 9 وفيات و18 إصابة مؤكدة بفيروس #كورونا في #واشنطن.\n",
            "-----\n",
            "مجلس الوزراء المصري يحظر دخول المواطنين القطريين، ضمن الإجراءات الإحترازية لمواجهة #فيروس_كورونا\n",
            "\n",
            "#مصر #قطر #فيروس_كورونا\n",
            "-----\n",
            "#عاجل | \"فرانس برس\": بافاريا أول مقاطعة ألمانية تفرض العزل على خلفية تفشي #كورونا\n",
            "-----\n",
            "مصر   vs   كورونا \n",
            "تابعوا النتيجة بهاليومين 🙂\n",
            "\n",
            "#للجهل_عنوان https://t.co/PSOLUxhRH5\n",
            "-----\n",
            "أنا مش هايهمني كورونا و لا بتاع و مش هلبس كمامه 😂😂😂و هناكل من شوكه واحده 😂😂😂 https://t.co/NooIhB9bhV\n",
            "-----\n",
            "#الكويت في عز أزمة #كورونا ..\n",
            "يداً تداوي شعبها ويداً تداوي شعوب العالم حفظهم الله وحفظ الكويت وشعبها من كل مكروه 🇰🇼❤️\n",
            "-----\n",
            "حتى كورونا ما عاقت معهم و سوو منها  فعاليات https://t.co/ab7utmLXvc\n",
            "-----\n",
            "@stc اتمنى اذا منتو راضين تردون ماتعلقون ابوي بالخط\n",
            "-----\n",
            "التوقع الأخير صحيح مص الدماء وعصر الجيوب  لمحدثك 10 سنوات اعانى من بنك الرياض https://t.co/GemzuekI7g\n",
            "-----\n",
            "ايطاليا تسجيل اكثر من 600 حالة وفاة في يوم واحد #كورونا\n",
            "#Covid_19 \n",
            "#ظل_بالبيت\n",
            "-----\n",
            "تخيل يجيلك كورونا على ايد ابن عرص من دول https://t.co/8gWRoNPgwJ\n",
            "-----\n",
            "في قلق في جروب اتحاد الملاك و تهزيق وكلام رايح جاي وموال\n",
            "-----\n",
            "يحيا الهلال مع الصليب 😂😂😍 و الله يعم عمر تسلملي عالكلام الحلو ده اكيد هشوفك قريب ❤❤ https://t.co/ZMbxCUAxjg\n",
            "-----\n",
            "حسابي باقي معلق على 12400 لهم شهور \" يحسبون اذا تابعوني تجيهم كورونا \n",
            "\n",
            "اهدى ياهوووه وادعمونا 😂🙏😍\n",
            "-----\n",
            "عاجل | جامعة جونز هوبكنز: ارتفاع عدد الوفيات بفيروس كورونا في الولايات المتحدة إلى 802 والإصابات إلى 55243\n",
            "-----\n",
            "لنا ربٌ رحيم ..\n",
            "متفائل جدا بانها ستزول قريبا 🙏\n",
            "\n",
            "#كورونا\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg2gf7Dnh--8",
        "outputId": "34aa535c-4451-4f5f-a4c5-c5c4f0b0b052"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "#processing UNTerm entries\n",
        "#from code_utils.align_utils import *\n",
        "from code_utils.arabic_lib import *\n",
        "from code_utils.general import *\n",
        "\n",
        "\n",
        "#op=requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)\n",
        "train_df = pd.read_csv(\"ArSarcasm_train.csv\")\n",
        "limit=40000\n",
        "all_ids=[]\n",
        "for i,a in enumerate(train_df.iterrows()):\n",
        "  row_dict=dict(iter(a[1].items())) \n",
        "  #print(row_dict)\n",
        "  tweet_txt=row_dict[\"tweet\"]\n",
        "  tweet_txt=tweet_txt.replace(\"_\",\" _ \")\n",
        "  tweet_txt_clean=clean_ar(tweet_txt)\n",
        "  print(row_dict[\"sentiment\"])\n",
        "  print(row_dict[\"dialect\"])\n",
        "  print(tok(tweet_txt_clean))\n",
        "\n",
        "  print(\"========\")\n",
        "  if i>100: break\n",
        "  # tw_id=str(a[1].Tweet_id)\n",
        "  # sentiment=a[1].sentiment\n",
        "  # all_ids.append(tw_id)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n",
            "gulf\n",
            "['\"', 'نصيحه', 'ما', 'عمرك', 'اتنزل', 'لعبة', 'سوبر', 'ماريو', 'مش', 'زي', 'ما', 'كنا', 'متوقعين', 'الله', 'يرحم', 'ايامات', 'السيقا', 'والفاميلي', '#', 'SuperMarioRun', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'نادين', '_', 'نسيب', '_', 'نجيم', '❤', '️', '❤', '️', '❤', '️', 'مجلة', '#', 'ماري', '_', 'كلير', '💭', '#', 'ملكة', '_', 'الصحراء', '👑', 'https', ':', '/', '/', 't', '.', 'co', '/', 'fVtn489TUS', '@', 'nadinenjeim', '@', 'MarieClaire', '_', 'AR', '\"']\n",
            "========\n",
            "neutral\n",
            "egypt\n",
            "['\"', '@', 'Alito', '_', 'NBA', 'اتوقع', 'انه', 'بيستمر', '\"']\n",
            "========\n",
            "neutral\n",
            "levant\n",
            "['\"', '@', 'KSA24', 'يعني', '\"', 'بموافقتنا', '\"', 'لأن', 'دمشق', 'صايرة', 'موسكو', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'RT', '@', 'alaahmad20', ':', 'قائد', 'في', 'الحرس', 'يعترف', 'بفقدان', 'السيطرة', 'الأمنية', 'في', 'شرقي', 'وغربي', 'إيران', '-', 'أحوازنا', '#', 'الأحواز', '#', 'السعودية', '@', 'abo', '_', 'asseel', '@', 'spagov', 'https', ':', '/', '…', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['شوال', 'الفلوس', 'سويرس', 'مشغول', 'اوى', 'اليومين', 'دول', 'بقناة', 'الجزيرة', 'ونازل', 'شتيمة', 'وتريقه', 'فيها', 'وفى', 'ضيوفها', 'خدك', 'بيوجعك', 'اوى', 'ياشوال']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'الأمين', 'العام', 'للأمم', 'المتحدة', ':', 'بشار', 'الأسد', 'قتل', '300', 'ألف', 'شخص', 'فى', 'سوريا', 'https', ':', '/', '/', 't', '.', 'co', '/', 'KMXrqNovgE', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['حسبنا', 'الله', 'ونعم', 'الوكيل', '\"', '#', 'انتخبوا', '_', 'العرص', '#', 'بيقولك', '_', 'أعرف', '_', 'بلدك', '#', 'الحرية', '_', 'للجدعان', 'ناس', 'عا', '…']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'RT', '@', 'isharatidriss', ':', '30', ')', '#', 'امريكا', '_', 'ومحاربة', '_', 'الارهاب', 'الذي', 'صنعتة', 'لمواصلة', 'الكيد', 'والحقد', 'الذي', 'يملاء', 'قلوب', 'دول', 'الكفر', 'علي', 'الاسلام', 'والمسلمين', '(', 'يا', 'ايها', 'الذي', '…', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'رئيس', 'لجنة', 'الحكام', 'رضا', 'البلتاجى', 'يفتح', 'النار', 'على', '#', 'عمرو', '_', 'أديب', 'https', ':', '/', '/', 't', '.', 'co', '/', '2PYLdCqvtp', '\"']\n",
            "========\n",
            "neutral\n",
            "egypt\n",
            "['\"', 'حتي', 'جوجل', 'مش', 'مصدق', 'اني', 'في', 'بيت', 'دمياط', '💔', 'https', ':', '/', '/', 't', '.', 'co', '/', 'GBTmGmMRiG', '\"']\n",
            "========\n",
            "negative\n",
            "gulf\n",
            "['\"', 'على', 'أي', 'اساس', 'سواقين', '#', 'اوبر', 'يعطون', 'الراكب', 'تقييم', '؟', 'انا', 'طبيعتي', 'ماتكلم', 'ولا', 'اسولف', 'مع', 'السواقين', 'حتى', 'سواقي', 'الخاص', 'اقوله', '…', '—', 'https', ':', '/', '/', 't', '.', 'co', '/', 'QEgjafZfrO', '\"']\n",
            "========\n",
            "neutral\n",
            "egypt\n",
            "['\"', '#', 'SuperMarioRun', 'من', 'جرب', 'اللعبة', '؟', 'انا', 'عجبتني', '👍', '🏻', '😍', '#', 'nintndo', 'https', ':', '/', '/', 't', '.', 'co', '/', 'bVvx7b9DH2', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['RT', '@', 'al', '_', 'rakkad', ':', 'مهما', 'كابر', 'البعض', '.', '.', 'الارجنتين', 'بحاجه', 'ماسه', 'الى', 'هذا', 'الرجل', '!', 'من', 'دونه', 'نتائج', 'سيئه', 'جدا', '.', '.', 'الارجنتين', 'من', 'دون', 'ميسي', 'لاشيء', '.', '.', 'https', ':', '/', '/', '…']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['الأسئلة', 'التي', 'هربنا', 'منها', 'أيام', 'المجلس', 'العسكري', 'لا', 'زالت', 'تطاردنا', 'وستظل', 'إلى', 'أن', 'نواجهها', 'ونجيب', 'عليها']\n",
            "========\n",
            "neutral\n",
            "levant\n",
            "['\"', 'انتي', 'أم', 'دفتر', 'وردي', 'عليه', 'ستيكر', 'ريال', 'مدريد', 'وكود', '؟', 'بالاداب', '—', 'اوووله', '😅', '😅', '،', '،', 'يمكن', 'اي', 'يمكن', 'لا', 'https', ':', '/', '/', 't', '.', 'co', '/', 'bSzbcCzqpi', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'هام', '#', 'حلب', 'بقي', 'في', 'الأحياء', 'الشرقية', 'لمدينة', 'حلب', 'حتى', 'اللحظة', 'حوالي', '2200', 'مسلح', 'مع', 'عائلاتهم', '.', 'و', 'من', 'المتوقع', 'أن', 'تكون', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'mBWllalZY4', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'عليهم', 'لعنة', 'الله', 'لن', 'تضيع', 'دماء', 'الابرياء', 'سوف', 'تحل', 'لعنة', 'السماء', 'على', 'ال', 'سعود', 'المجرمين', 'ومن', 'حالفهم', '#', 'الضنيه', '#', 'Christmas', '#', 'وينن', 'https', ':', '/', '/', 't', '.', 'co', '/', 'rpMV6wHzSu', '\"']\n",
            "========\n",
            "neutral\n",
            "levant\n",
            "['\"', 'صارلي', 'ثلاث', 'اسابيع', 'انطر', 'هالتنبيه', '😭', '😭', '😭', '#', 'SuperMarioRun', 'https', ':', '/', '/', 't', '.', 'co', '/', '4S3UdcQwn2', '\"']\n",
            "========\n",
            "neutral\n",
            "gulf\n",
            "['\"', 'حتي', 'غوغل', 'شاهد', '.', '.', '.', '!', 'https', ':', '/', '/', 't', '.', 'co', '/', '5EdQLrnS0d', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', '#', 'الحوثي', '#', 'المخلوع', 'عجبا', 'ل', '#', 'كيري', 'و', '#', 'قابوس', 'كيف', 'يتجاهلاالقرار2216', 'الصادرمن', 'مجلس', 'الأمن', 'ومحاولة', 'الالتفاف', 'عليه', '؟', 'هل', 'هذه', 'الحياديةوالسعي', 'لمصلحة', '#', 'اليمن', '\"']\n",
            "========\n",
            "positive\n",
            "egypt\n",
            "['\"', 'شغل', 'جامد', '👌', '👍', '#', 'MannequinChallenge', 'https', ':', '/', '/', 't', '.', 'co', '/', 'r0aO699OxP', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'كل', 'يوم', '-', '#', 'السيسي', 'يجتمع', 'اليوم', 'مع', 'المجلس', 'الاعلى', 'للاستثمار', '#', 'كل', '_', 'يوم', '|', '#', 'ON', '_', 'E', '|', '#', 'عمرو', '_', 'اديبhttps', ':', '/', '/', 't', '.', 'co', '/', 'vhmqGNmZdQ', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'نيويورك', 'تايمز', ':', 'على', 'أوباما', 'وقف', 'دعمه', 'للسعودية', 'إذا', 'لم', 'توقف', 'مذابحها', 'في', '#', 'اليمن', 'و', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', '44msnSZTWi', '#', 'اليمن', '#', 'اليمن', '_', 'الان', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['#', 'توفيق', '_', 'عكاشة', 'بيقول', 'على', 'هيفا', 'ولية', 'ناشفة', 'امال', 'حياة', 'الدرديري', 'بتاعتك', 'دي', 'ايه', '؟', '؟', 'يافلاح', 'ميت', 'الغرقا', 'ياحماررر']\n",
            "========\n",
            "neutral\n",
            "egypt\n",
            "['\"', 'RT', '@', 'yousefalawnah', ':', 'معمم', 'آخر', 'موديل', 'شغل', 'غوتشي', '😂', '😂', '😂', 'https', ':', '/', '/', 't', '.', 'co', '/', 'F2mOrNE1Pj', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'RT', '@', 'al', '_', 'tagreer', ':', 'هل', 'تغير', 'إيران', 'من', 'سياسات', 'تعاملها', 'مع', 'السعودية', '؟', 'https', ':', '/', '/', 't', '.', 'co', '/', 'tgQbfq0nMLالرابط', 'البديل', ':', 'https', ':', '/', '/', 't', '.', 'co', '/', 'KbRifMgqaA', 'https', ':', '/', '/', 't', '.', 'co', '/', 'D', '…', '\"']\n",
            "========\n",
            "negative\n",
            "levant\n",
            "['\"', '#', 'حلب', '#', 'سورياو', 'ايه', 'مقابل', 'ال', 'بتعمله', '#', 'روسيا', 'في', 'سورياكل', 'الروس', 'ال', 'بيتقتلوا', 'و', '.', 'كل', 'طلعة', 'جويةاكيد', 'مش', 'لله', 'و', 'سوريا', 'و', 'بشار', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['كل', 'عام', 'وأنتم', 'بخير', 'وكل', 'منا', 'في', 'سلام', 'مع', 'نفسه', 'ومع', 'الغير', 'وفي', 'وطن', 'يتسع', 'للجميعمازالت', 'قناعتي', 'أن', 'شباب', 'مصر', 'الثورة', 'هم', 'الحل', 'والأمل', ':', 'فكر', 'خلاق', 'وقيادة', 'فتية']\n",
            "========\n",
            "negative\n",
            "levant\n",
            "['\"', '@', 'nsalmk', 'ونيكي', '?', 'يمه', 'امزح', 'لاجد', 'وربي', 'بيونسيه', 'هي', 'الي', 'وحيده', 'تظبط', 'هالشي', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['مقال', 'رائع', 'لخالد', 'منتصر']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', '@', 'MajdKanaann', 'اللجان', 'الثوريه', 'هي', 'عصابه', 'تابعه', 'لايران', 'وامريكا', 'وبريطانيا', 'زي', 'حقه', 'حزب', 'الله', 'لبنان', 'وعمهم', 'الكبير', 'نتن', 'ياهو', 'الكلب', 'والله', 'يسترع', 'المسلمين', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'RT', '@', 'kokokolove45', ':', 'شاهد', 'الشووط', '1', 'يوتيوب', 'من', 'مباراة', '#', 'الارحنتين', '_', 'باراجواي', 'يوتيوبhttps', ':', '/', '/', 't', '.', 'co', '/', 'Gz8AYcscerجوالhttps', ':', '/', '/', 't', '.', 'co', '/', 'Gz8AYcscer', '…', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['\"', '#', 'غوتشي', '\"', 'فلورا', '\"', 'النسائيمع', 'نغمات', 'عطرية', 'أكثر', 'عمقا', 'وجاذبيةاحصلي', 'عليه', 'من', '#', 'نقطة', '_', 'الأناقة', 'الآن', 'في', '#', 'أملج', 'https', ':', '/', '/', 't', '.', 'co', '/', 'bM0GjfImCN', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'اليمنية', '|', '18', 'شهيدا', 'و27', 'مصابا', 'في', '#', 'خروقات', '_', 'الإنقلابيين', 'بتعز', 'خلال', '24', 'ساعةhttps', ':', '/', '/', 't', '.', 'co', '/', 'DMa8xnikF3', '#', 'التحالف', '_', 'العربي', '_', 'سند', '_', 'اليمن', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'أعادت', 'أمازون', 'تهيئة', 'كلمات', 'المرور', 'للعديد', 'من', 'مستخدميها', 'بعد', 'تسرب', 'قائمة', 'على', 'الإنترنت', 'تضم', 'بريدا', 'إلكترونيا', 'وكلمات', 'مرور', '،', 'و', 'https', ':', '/', '/', 't', '.', 'co', '/', 'rk9hhGU8mL', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'عشرات', 'الجرحى', 'في', 'احتجاجات', 'على', 'مشروع', 'لأنبوب', 'نفط', 'في', 'الولايات', 'المتحدة', 'https', ':', '/', '/', 't', '.', 'co', '/', 'Um1sVbYzS1', '#', 'سبوتنيك', '#', 'فلسطين', '#', 'القدس', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['اطاح', 'الاستفتاء', 'بمبدأ', 'تكافؤ', 'الفرص', 'حينما', 'لم', 'يسمح', 'بالتصويت', 'لكل', 'المقيمين', 'بالخارج', 'الا', 'من', 'قام', 'بالتسجيل', 'في', 'الانتخابات']\n",
            "========\n",
            "neutral\n",
            "egypt\n",
            "['\"', '@', 'abo3maira', '@', 'loaiomranمش', 'عارفصورة', 'وجدتها', 'علي', 'فيسبوك', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'الجيش', '_', 'السوري', 'يسيطر', 'على', 'طرق', 'امداد', 'المسلحين', 'القادمة', 'من', 'جهة', 'حزرما', 'باتجاه', 'بلدة', 'الميدعاني', 'بالغوطة', 'الشرقية', 'في', 'ريف', '#', 'دمشق', '.', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '88', '%', '\"', 'من', 'قضايا', 'الابتزاز', 'في', 'السعودية', 'سببها', 'مقاطع', 'وصور', 'البنات', '#', 'الجوف', '#', 'دومة', '#', 'القريات', '#', 'سكاكا', '#', 'سكاكا', '_', 'ال', '…']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['\"', 'تصور', 'يا', 'حفتر', 'عملية', 'مثل', 'بنيان', 'مرصوص', 'أعتمادهم', 'علي', 'الله', '!', '!', 'لا', 'أبناء', 'زايد', 'و', 'لا', 'بوتن', 'و', 'لا', 'فرنسا', 'و', 'لا', 'داعش', '!', '!', 'فهل', 'يقوي', 'أحد', 'أن', 'يغلب', 'الله', '☝', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['\"', 'زينة', 'الميلاد', 'بجبيل', 'الحلوة', '❤', '️', '#', 'jbeil', '#', 'livelovelebanon', '#', 'christmas', '#', 'byblos', '#', 'livelovebyblos', '#', 'lebanon', '…', 'https', ':', '/', '/', 't', '.', 'co', '/', 'eA9RnTcRBS', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'تعرف', 'على', 'تاريخ', 'استخدام', '#', 'روسيا', 'و', '#', 'الصين', 'لحق', 'نقض', '#', 'الفيتوتقرير', ':', 'زاهر', 'علي', '#', 'اتحاد', '_', 'الشبكات', '_', 'الاسلامية', 'https', ':', '/', '/', 't', '.', 'co', '/', '4PMGyuVMI8', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['كذبة', 'حقيرة', 'الاهرام', 'العربي', ':', 'مرسي', 'طبع', '65', 'مليار', 'جنيه', 'والبنك', 'المركزى', 'يأسف', 'ويستنكر', 'ويرفض', 'التعقيب']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['\"', 'RT', '@', 'amer', '_', 'alkubaisi', ':', 'حيدر', 'العبادي', 'حذف', 'منشور', 'انتقاد', 'أردوغان', 'من', 'الفيس', 'بوك', '،', 'وربما', 'بعد', 'قليل', 'يحذفه', 'من', 'تويتر', '،', 'لنتابع', 'اللعبة', '.', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['\"', '21', 'نوفمبر', 'المجد', 'التليد', 'لرقي', 'فن', 'فيروز', '❤', '️', '🎼', '#', 'فيروز', 'https', ':', '/', '/', 't', '.', 'co', '/', 'v2JvjGAiNx', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['#', 'مرسى', '_', 'رئيسى', '#', 'مرسي', '_', 'رئيسي', '#', 'انتخبوا', '_', 'البرص', '#', 'انتخبوا', '_', 'العرص', 'آمال']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'RT', '@', '3FLnQe', ':', '#', 'نتيجه', '_', 'الاخضر', '_', 'مع', '_', 'الامارات', 'آخر', 'فوز', 'للامارات', 'على', 'السعودية', 'كان', 'ميسي', 'يملك', '0', 'اهداف', 'على', 'ريال', 'مدريدوالان', 'الهداف', 'التاريخي', 'لليغا', 'و', 'ل', '…', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', '@', 'riyadhalasaad', 'عندما', 'نشاهد', 'مايجري', 'في', 'سوريا', 'من', 'قتل', 'للإطفال', 'والنساء', 'من', 'قوات', 'الأسد', 'وطيران', 'المجرم', 'بوتن', 'تقطر', 'قلوبنا', 'دمانحن', 'نشارككم', 'الألم', 'والعذاب', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'Listen', 'to', 'أشتقتلك', '-', 'رامى', 'عياش', '#', 'SoundCloudhttps', ':', '/', '/', 't', '.', 'co', '/', 'oqCGhGGQXS', '\"']\n",
            "========\n",
            "positive\n",
            "magreb\n",
            "['\"', 'ما', 'تغيب', 'كثير', 'عنا', '؛', 'غيبتك', 'عم', 'تقتلنا', 'حبيب', 'القلب', 'توحشتك', 'بزاف', '#', 'رامي', '_', 'عياش', '#', 'أمير', '_', 'الليل', '@', 'RamyAyach', '\"']\n",
            "========\n",
            "negative\n",
            "levant\n",
            "['\"', 'كل', 'مرة', 'جورج', 'ياسمين', 'بقدم', 'الاخبار', 'بحط', 'ايدي', 'ع', 'قلبي', 'ليقطع', 'قطوع', 'الرئيس', 'ميشال', '.', '.', '.', '.', '.', 'عون', '😟', '😳', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['عقول', 'زباله', 'تسئ', 'لرجل', 'كبر', 'اجدادهم', 'فعلا', 'العرب', 'تربيتهم', 'صفر', '#', 'بالعربي']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['من', 'أعذب', 'مفردات', '#', 'الحياة', 'وأجملها', 'هو', '#', 'اعطاء', 'العذر', 'لغياب', 'البعض', '!', '[', '[', 'لان', 'سيد', '#', 'البشر', 'تحدث', 'عن', '#', 'التماس', 'العذر', ']', ']', '،', 'صلوات', 'الله', 'ع', '…']\n",
            "========\n",
            "negative\n",
            "gulf\n",
            "['\"', '@', 'mohmad3033', 'والله', 'مهو', 'كلهم', 'يبو', 'سلطان', '.', '.', 'الاعلام', 'الغربي', 'يبي', 'يصور', 'لنا', 'ان', 'الشيعة', 'خطر', 'ويبي', 'يصور', 'للشيعة', 'أنفسهم', 'ان', 'السنة', 'خطر', 'عليهم', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'RT', '@', 'alhayat', '_', 'syria', ':', 'المعارضة', 'تسترجع', 'مواقع', 'في', 'حلب', '.', '.', '.', 'وروسيا', 'تثبت', '«', 'قاعدة', 'عسكرية', 'دائمة', '»', 'في', 'طرطوسhttps', ':', '/', '/', 't', '.', 'co', '/', 'R2LYVG5qyf', 'https', ':', '/', '/', 't', '.', 'co', '/', '7X5V6OVY', '…', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'أردوغان', ':', 'حان', 'الوقت', 'لفتح', 'صفحةجديدةفي', 'العلاقات', 'بين', '#', 'إسرائيل', 'و', '#', 'تركيا', 'https', ':', '/', '/', 't', '.', 'co', '/', 'o3mPATmiGi', '#', 'فلسطين', '#', 'غزة', '#', 'حماس', '#', 'السعودية', '#', 'مصر', '#', 'العراق', '#', 'لبنان', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['سبحان', 'المعز', 'المذل', 'قصة', 'علي', 'العريض', 'من', 'السجن', 'إلي', 'وزير', 'داخلية', 'تونس']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'أخبار', 'رياضية', ':', 'هذا', 'ما', 'قاله', 'رونالدو', 'عن', 'مشاركة', 'ريال', 'مدريد', 'في', 'كأس', 'العالم', 'للأندية', 'https', ':', '/', '/', 't', '.', 'co', '/', 'wFsQAJktAR', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '@', 'Sllokei', 'اسطورة', 'التنس', 'روجر', 'فيدرر', 'وصاحب', '88', 'لقب', 'منها', '17', 'جراند', 'سلام', '#', 'اسطورة', '❤', '️', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'ميزة', 'في', 'هاتف', '«', 'جوجل', '»', 'تكشف', 'ابتزاز', '«', 'أبل', '»', 'https', ':', '/', '/', 't', '.', 'co', '/', 'ZtFyfDk79x', 'https', ':', '/', '/', 't', '.', 'co', '/', '6ygRaXjIvq', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['إلى', 'حد', 'البكاء', 'على', 'يديك', '،', 'أحاول', 'أن', 'أغني', '،', 'و', 'أعرف', 'أنني', 'وطن', 'و', 'منفى', '،', 'و', 'أني', 'لست', 'مني', '،', 'و', 'لكني', '،', 'أحاول', 'أن', 'أغني', '،', 'حين', 'أبكي']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'مقتل', 'سبعة', 'أشخاص', 'إثر', '#', 'حريق', 'غابات', 'اندلع', 'في', 'منطقة', 'جبلية', 'سياحية', 'جنوبي', '#', 'الولايات', '_', 'المتحدة', '#', 'أ', '_', 'ف', '_', 'ب', 'https', ':', '/', '/', 't', '.', 'co', '/', 'mMqXoe47Mn', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['اعرف', 'بلادك', '-', 'المغرب', 'عين', 'أسردون', '-', 'توجد', 'في', 'بني', 'ملال', 'وهي', 'مدينة', 'مغربية', 'تقع', 'في', 'الوسط', 'الغربي', 'للمملكة', 'المغربية', 'وسهل']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['\"', 'تايملاين', 'يليق', 'بيه', 'فيلم', '\"', 'الأرهاب', 'والكباب', '\"', 'بدخلة', 'يسرا', 'المفاجأة', '😌', '😃', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'مع', 'كامل', 'محبتي', 'الشخصية', 'للرؤساء', 'عون', 'وبري', 'والحريري', '،', 'لا', 'تفهموني', 'خطأ', '؟', '؟', '؟', 'عندما', 'يقبل', 'الرئيس', 'ميشال', 'عون', 'بأن', 'ينتخب', 'من', 'قبل', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'U0ffFBkR7u', '\"']\n",
            "========\n",
            "neutral\n",
            "gulf\n",
            "['\"', 'ممتنة', 'للثريد', 'وكايسو', 'انهم', 'يفتحون', 'نفسك', '💛', 'وايوه', 'بوكيمون', 'تمام', '،', 'كيف', 'حبيبة', 'البوكيمون', 'طيب', '؟', 'يومي', 'مذهلرائع', 'شكرا', 'لك', '،', 'احصلي', '…', 'https', ':', '/', '/', 't', '.', 'co', '/', 'ARa0PNnYJ6', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['في', 'ناس', 'غاويا', 'كدب', 'وكمان', 'بتخاف', 'فيديو', 'متفبرك', 'تماما', 'مندوبتنا', 'في', 'ليسيه', 'الحرية', 'مادخلتش', 'اللجنة', 'أصلا', 'بس', 'حركات', 'مفضوحة']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'RT', '@', 'Elaph', ':', 'طيران', 'الإمارات', 'وغوغل', 'يطلقان', 'مبادرة', 'تثقيفية', '#', 'طيران', '_', 'الإمارات', '#', 'غوغل', 'https', ':', '/', '/', 't', '.', 'co', '/', 'zfBZqiEPkW', 'https', ':', '/', '/', 't', '.', 'co', '/', 'dR1aX9RtI5', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'سبع', 'دول', 'من', 'تدعم', 'داعش', 'امريكا', 'بريطانيا', 'فرنسا', 'إسرائيل', 'تركيا', 'السعوديه', 'قطر', '\"']\n",
            "========\n",
            "negative\n",
            "levant\n",
            "['\"', '#', 'النازحين', '_', 'السوريين', 'بيعتبرو', 'اللي', 'صار', 'ب', '#', 'حلب', 'تحرير', 'ومبسوطين', 'وعم', 'بهللو', '#', 'بس', '_', 'هيك', '😊', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'تركيا', 'تنفي', 'عقد', 'صفقة', 'مع', 'روسيا', 'بشأن', 'حلب', 'https', ':', '/', '/', 't', '.', 'co', '/', 'XX4MMEgtW5', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['\"', 'RT', '@', 'aawsat', '_', 'News', ':', '#', 'نجاح', '_', 'الحج1437ه', '|', 'النجاحات', '#', 'السعودية', 'المتوالية', 'في', 'تنظيم', '#', 'الحج', 'تلجم', 'أفواه', 'المشككينhttps', ':', '/', '/', 't', '.', 'co', '/', 'Ovq7tMPWda', 'https', ':', '/', '/', 't', '.', 'co', '/', 'dy', '…', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'RT', '@', 'AbuAltahish6', ':', '@', 'hrw', '_', 'ar', '@', 'aboaqil2010', 'امريكا', 'هي', 'راعية', 'الارهاب', '.', 'وتحالفها', 'الدولي', 'لضرب', 'داعش', 'انما', 'هو', 'لدعم', 'داعش', 'وكبيرتها', 'السعودية', 'العالم', 'يعرف', '…', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['#', 'يلا', '_', 'نحفل', '_', 'علي', '_', 'احمد', '_', 'ادم', 'الرد', 'هشتاج', 'بردوا', '#', 'باسم', '_', 'طلع', '_', 'حرامى']\n",
            "========\n",
            "neutral\n",
            "gulf\n",
            "['\"', '@', 'Ahmed', '_', 'ALHasani', 'ويندوز', '10', 'ممتاز', 'بس', 'جلب', 'مشاكل', 'واجد', 'في', 'لاب', 'توب', 'منها', 'الاضاة', 'الي', 'حلتها', 'ومنها', 'انه', 'الجهاز', 'يعلق', 'على', 'فترات', 'اذا', 'تركته', 'من', 'دون', 'استخدام', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', '@', 'DimaSadek', 'الفرق', 'بين', 'الصورتين', 'هي', 'صورة', 'لاهالي', 'الجنوب', 'في', 'حرب', 'تموزوالصوره', 'الثانية', 'لنزوح', 'أهالي', 'حلب', 'الفرق', 'النازحين', 'السور', '…', 'https', ':', '/', '/', 't', '.', 'co', '/', 'NC2lPOKHke', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['\"', 'وقالو', 'سعيدة', 'في', 'حياتها', '😣', '#', 'إليسا', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['إذاغيب', 'الموت', '\"', 'نعناع', 'الجناين', '\"', 'فإن', 'حكاء', 'مصرالأصيل', 'سيظل', 'حاضرافى', 'الوجدان', 'المصرى', 'بأعماله', 'التى', 'جسدت', 'أحلام', 'المهمشين', '،', 'رحم', 'الله', '\"', 'صياد', 'اللولى', '\"', 'خيرى', 'شلبى']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['#', 'علوم', '_', 'وتكنولوجياغوغل', 'لا', 'تعين', 'أحدا', 'فوق', 'ال40', 'عاما', 'للعمل', 'لديهاأقامت', 'مجموعة', 'من', 'الباحثين', 'عن', 'عمل', '،', 'وتفوق', 'أعمارهم', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'jQoCRYsLaK']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'RT', '@', 'mstr', '_', 'federer', ':', '@', 'fed89erer', 'عالم', 'الكرة', 'الصفراء', 'كان', 'يتيما', 'حتى', 'جاء', 'فيدرر', 'و', 'اخرج', 'جمال', 'هذه', 'الرياضة', 'للعالم', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['ف', '#', 'مصر', '#', 'شخصية', '_', 'مريبة', 'متعرفش', '#', 'بيشتغل', 'لحساب', 'مين', 'بالظبط', 'اسمو', '#', 'نجيب', '_', 'ساويرس', 'بيعمل', '#', 'قناة', '#', 'لجاسوس', 'اسمه', '#', 'يسرى', '_', 'فودة', 'ليستضيف', '#', 'عميل', '_', 'برخصة', 'اسمه', '#', 'جمال', '_', 'عيد']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', '#', 'ميشيل', '_', 'تويني', ':', 'كيف', 'تم', 'إخفاء', 'معالم', 'من', 'موقع', 'التفجير', 'عند', 'إغتيال', '#', 'جبران', '_', 'تويني', 'ومن', 'المسؤول', 'ولماذا', 'لم', 'يحرك', 'الملف', 'كل', 'هذا', 'الوقت', '؟', '@', 'MichelleTueini', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'لافروف', ':', 'إلغاء', 'اجتماع', '#', 'جنيف', 'حول', '#', 'حلب', 'بسبب', 'عدم', 'مشاركة', '#', 'واشنطن', 'https', ':', '/', '/', 't', '.', 'co', '/', 'ozXKurx5dr', '\"']\n",
            "========\n",
            "negative\n",
            "gulf\n",
            "['\"', '@', 'LaaHooB', '_', 'Q8', '@', 'alhurranews', 'خلاص', 'طاح', 'الحطبو', 'بوكيمون', 'يبدى', 'قلقه', '\"']\n",
            "========\n",
            "positive\n",
            "msa\n",
            "['لمصر', 'شعب', 'يحميها', 'رب', 'أجعل', 'هذا', 'البلد', 'آمنا', 'مطمئنا']\n",
            "========\n",
            "negative\n",
            "levant\n",
            "['\"', 'إثر', 'هجومهما', '\"', 'الهمجي', '\"', 'على', '#', 'حلب', '.', '.', 'البيت', 'الأبيض', 'يحذر', 'الأسد', 'و', '#', 'روسيا', '#', 'سوريا', '#', 'واشنطن', '#', 'روسيا', '_', 'تحرق', '_', 'حلب', '#', 'تدمير', '_', 'المشافي', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'علي', 'الأمين', 'يكتب', ':', 'موسكو', 'أسقطت', 'هدنة', 'حلب', ':', 'تريد', 'سوريا', 'كلها', 'لا', '‘', 'المفيدة', \"'\", 'فقط', 'https', ':', '/', '/', 't', '.', 'co', '/', 'mNFUsKJjVJ', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', '#', 'عرب48', '|', 'تطويق', 'الأخبار', 'الكاذبة', 'على', '#', 'فيسبوك', ':', 'تهدد', 'الديمقراطية', 'https', ':', '/', '/', 't', '.', 'co', '/', 'jxeLkHwWy0', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'بعد', 'الإنتخابات', 'الأمريكية', 'الأخيرة', 'أثبت', 'الإعلام', 'الغربي', 'أنه', 'إعلام', 'موجه', 'وغير', 'صادق', 'وأثبت', 'الإعلام', 'العربي', 'أنه', 'ببغائي', 'بامت', '…', 'https', ':', '/', '/', 't', '.', 'co', '/', 'jBCZQ61Dkx', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['فى', 'مصر', 'الانقلاب', 'تعلو', 'اصوات', 'العهر', 'والدعارة', 'وتخفض', 'اصوات', 'الحق', 'والعدل', 'والشرفاء', 'تحكمهم', 'بيادة', 'تلاحقهم', 'رصاصة', 'تأويهم', 'سجون']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'موقع', 'كلمتي', ':', 'دفاع', 'شفيق', ':', 'لم', 'تعد', 'هناك', 'عقبات', 'تمنعه', 'من', 'العودة', 'لمصر', '#', 'احمد', '_', 'شفيق', 'https', ':', '/', '/', 't', '.', 'co', '/', 'gfCaCi0RwW', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'الجيش', '_', 'السوري', 'يدمر', '3', 'جرافات', 'ويقتل', 'من', 'فيها', 'جنوب', 'مقبرة', 'حوش', 'نصري', 'في', '#', 'الغوطة', '_', 'الشرقية', '\"']\n",
            "========\n",
            "negative\n",
            "levant\n",
            "['\"', 'النازحين', 'السوريين', 'منحاشين', 'من', 'قتل', 'بشار', 'وحزب', 'الله', 'وايران', 'لهمراحوا', 'حق', 'لبنان', 'اللي', 'تحكمها', 'ايران', 'وحزب', 'اللهدخلك', 'راح', 'يعالجونهم', 'بفلوس', 'الكويت', '؟', '!', '!', '\"']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', '#', 'فريق', '_', 'الاصائل', '_', 'التطوعي', '-', 'ينظم', 'حفل', '#', 'المولد', '_', 'النبوي', '_', 'الشريف', 'https', ':', '/', '/', 't', '.', 'co', '/', 'RgyigWzAHq', '\"']\n",
            "========\n",
            "negative\n",
            "egypt\n",
            "['\"', 'ياويلللي', 'اعوذ', 'بالله', '#', 'ماذا', '_', 'لو', '_', 'المدارس', '_', 'مختلطه', '\"']\n",
            "========\n",
            "neutral\n",
            "egypt\n",
            "['#', 'مصر', '•', 'كاريكاتير', '_', '#', 'انتخبوا', '_', 'العرص']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['بلغ', 'التدني', 'بالبعض', 'حد', 'وضع', 'صورتي', 'واسمي', 'على', 'حسابات', 'تويتر', 'مزيفة', 'وإستخدامها', 'للتطاول', 'على', 'حبيبتي', 'وزوجتي', 'وللخوض', 'في', 'عرضي', '،', 'وتداولها', 'عبر', 'مواقع', 'غير', 'جادة']\n",
            "========\n",
            "positive\n",
            "egypt\n",
            "['#', 'رمضان', '_', 'الخير', 'قرب']\n",
            "========\n",
            "neutral\n",
            "msa\n",
            "['\"', 'كارلو', 'أنشلوتي', 'قبل', 'قليل', ':', 'ريال', 'مدريد', 'هو', 'نادي', 'الكرة', 'الأرضية', '!', '.', '.', '.', '#', 'تصريح', '_', 'قوي', '👌', '#', 'حقيقة', '#', 'Moh7', '\"']\n",
            "========\n",
            "negative\n",
            "msa\n",
            "['\"', 'RT', '@', 'alsrkhyalhasany', ':', '#', 'الصرخي', '_', 'يكشف', '_', 'زيف', '_', 'المدلسينانحرف', 'السنة', 'والشيعة', 'بسبب', 'داعش', 'والسيستانيمقتبس', 'من', 'بحث', '(', 'السيستاني', 'ما', 'قبل', 'المهد', 'الى', 'ما', 'بعد', 'الل', '…', '\"']\n",
            "========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VJjTsWgAdsj"
      },
      "source": [
        "#now let's prepare a submission\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "import os\n",
        "from code_utils.align_utils import *\n",
        "from code_utils.arabic_lib import *\n",
        "from code_utils.general import * \n",
        "from itertools import groupby\n",
        "from collections import Counter\n",
        "\n",
        "ar_counter_dict=load_counts(\"ar_count.txt\", tmp_count_dict={})\n",
        "\n",
        "#op=requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)\n",
        "test_df = pd.read_csv(\"test1_with_text.csv\")\n",
        "#limit=40000\n",
        "all_ids=[]\n",
        "prediction_list=[]\n",
        "sentiment_list=[]\n",
        "for i,a in enumerate(test_df.iterrows()):\n",
        "  rnn.zero_grad()\n",
        "  if i%20==0: print(i)\n",
        "  row_dict=dict(iter(a[1].items())) \n",
        "  #print(row_dict)\n",
        "  tweet_id=row_dict[\"Tweet_id\"]\n",
        "  tweet_txt=row_dict[\"Text\"]\n",
        "  tweet_txt=tweet_txt.replace(\"_\",\" _ \")\n",
        "  tweet_txt_clean=clean_ar(tweet_txt)\n",
        "  tmp_words=tok(tweet_txt_clean)\n",
        "  ar_words=tok_ar(tmp_words,ar_counter_dict)\n",
        "  #print(tweet_txt_clean)\n",
        "  sentiment=0\n",
        "  for word in ar_words:\n",
        "    if \"زفت\" in word: sentiment=-1\n",
        "    if \"يلعن\" in word: sentiment=-1\n",
        "    if \"ملعون\" in word: sentiment=-1\n",
        "    if \"سيء\" in word: sentiment=-1\n",
        "    if \"أكره\" in word: sentiment=-1\n",
        "    if word.split(\"_\")[-1] in [\"خرا\",\"خراء\",\"خرائي\"]: sentiment=-1\n",
        "    if \"فخور\" in word: sentiment=1\n",
        "    # if \"زين\" in word: sentiment=1\n",
        "    #if \"عظيم\" in word: sentiment=1\n",
        "  rnn_output=predict(tweet_txt)\n",
        "  cur_pred_list=pred2label(rnn_output,outcomes)\n",
        "  sentiment=cur_pred_list[0][0]\n",
        "\n",
        "  # print(tweet_txt)\n",
        "  # print(pred)\n",
        "  # print(\"------\")\n",
        "  \n",
        "  # if sentiment==1:\n",
        "  #   print(tweet_txt_clean)\n",
        "  #   print(\"-----\")\n",
        "  prediction_list.append((tweet_id,sentiment))\n",
        "  sentiment_list.append(sentiment)\n",
        "\n",
        "print(Counter(sentiment_list))\n",
        "#now writing the predictions\n",
        "import csv\n",
        "\n",
        "cur_fname=\"submission2.csv\"  \n",
        "# field names \n",
        "fields = ['Tweet_id', 'sentiment']    \n",
        "# data rows of csv file \n",
        "rows= prediction_list\n",
        "with open(cur_fname, 'w') as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerow(fields)\n",
        "    write.writerows(rows)\n",
        "\n",
        "\n",
        "# rows = [ ['Nikhil', 'COE', '2', '9.0'], \n",
        "#          ['Sanchit', 'COE', '2', '9.1'], \n",
        "#          ['Aditya', 'IT', '2', '9.3'], \n",
        "#          ['Sagar', 'SE', '1', '9.5'], \n",
        "#          ['Prateek', 'MCE', '3', '7.8'], \n",
        "#          ['Sahil', 'EP', '2', '9.1']] \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuryOzYJPs-K",
        "outputId": "1002a57c-3346-4ee2-dbbe-315fdaa3ffb9"
      },
      "source": [
        "#Let's use BERT multilingual model to extract the embeddings\n",
        "import itertools\n",
        "src=\"I do understand nothing\"\n",
        "src=\"أنا مش فاهم حاجة\"\n",
        "src=\"ربما الموت يقترب مني وانا لا اشعر به لطفك يا الله في سكرة موتي ان تكون خاتمتي حسنه ثم الجنه\"\n",
        "align_layer = 8\n",
        "\n",
        "sent_src=src.strip().split()\n",
        "token_src= [tokenizer.tokenize(word) for word in sent_src] #to use the bert tokenizer\n",
        "#token_src= [[word] for word in sent_src] #just to use the original tokenization\n",
        "wid_src= [tokenizer.convert_tokens_to_ids(x) for x in token_src]\n",
        "ids_src = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids']\n",
        "out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "\n",
        "print(\"token_src\",token_src)\n",
        "print(\"wid_src\",wid_src)\n",
        "print(\"ids_src\", ids_src)\n",
        "print(out_src.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token_src [['ر', '##ب', '##ما'], ['ال', '##موت'], ['ي', '##قت', '##رب'], ['من', '##ي'], ['و', '##انا'], ['لا'], ['اش', '##عر'], ['به'], ['ل', '##ط', '##ف', '##ك'], ['يا'], ['الله'], ['في'], ['س', '##كرة'], ['موت', '##ي'], ['ان'], ['تكون'], ['خ', '##ات', '##مت', '##ي'], ['حسن', '##ه'], ['ثم'], ['ال', '##جن', '##ه']]\n",
            "wid_src [[773, 11086, 14495], [59901, 67538], [793, 39053, 30075], [10289, 10461], [791, 37641], [13879], [92118, 69861], [10327], [787, 14286, 11687, 12497], [60844], [15764], [10210], [775, 62708], [83131, 10461], [14269], [24027], [770, 10564, 34783, 10461], [27304, 10388], [15902], [59901, 56760, 10388]]\n",
            "ids_src tensor([  101,   773, 11086, 14495, 59901, 67538,   793, 39053, 30075, 10289,\n",
            "        10461,   791, 37641, 13879, 92118, 69861, 10327,   787, 14286, 11687,\n",
            "        12497, 60844, 15764, 10210,   775, 62708, 83131, 10461, 14269, 24027,\n",
            "          770, 10564, 34783, 10461, 27304, 10388, 15902, 59901, 56760, 10388,\n",
            "          102])\n",
            "torch.Size([39, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSHa1urGQTy5",
        "outputId": "30ce8c90-1e56-430e-8d24-6773b207dd27"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wid_src [[146], [10149], [49151]]\n",
            "ids_src tensor([  101,   146, 10149, 49151,   102])\n",
            "torch.Size([3, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTrCswwwQVHF"
      },
      "source": [
        "import h5py\n",
        "hdf5_fpath=os.path.join(exp_dir,\"data.hdf5\")\n",
        "hdf5_file=h5py.File(hdf5_fpath, 'w')\n",
        "grp = hdf5_file.create_group(it)\n",
        "   # for item in hdf5_file: \n",
        "    #     obj=hdf5_file[item]\n",
        "    #     for ds in obj:\n",
        "    #         cur_data=obj[ds]\n",
        "    #         print(item, ds, cur_data.shape, cur_data.attrs[\"uas\"])\n",
        "        #print(item)\n",
        "dset = grp.create_dataset(example_id, data=one_hot_tensor_torch, compression=\"gzip\")\n",
        "dset.attrs[\"uas\"]=parser_uas\n",
        "dset.attrs[\"sent_size\"]=sent_size        \n",
        "hdf5_file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCGGXUCkgwGd"
      },
      "source": [
        "device_num=1\n",
        "\n",
        "if device_num==0: device_name=\"cuda:0\"\n",
        "elif device_num==1: device_name=\"cuda:1\"\n",
        "else: device_name=\"cpu\"\n",
        "\n",
        "\n",
        "#device = torch.device(device_name if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device('/device:GPU:0')\n",
        "#device = torch.device('device:GPU:0')\n",
        "device = torch.device('cuda')\n",
        "\n",
        "embeds=embeds.to(device)\n",
        "def init_hidden(self):\n",
        "  return (torch.zeros(1, self.batch_size, self.hidden_size).to(device),\n",
        "      torch.zeros(1, self.batch_size, self.hidden_size).to(device)) \n",
        "rnn = RNN(n_input, n_hidden, 1)\n",
        "rnn.to(device)\n",
        "\n",
        "category_tensor=torch.tensor(correct_heads, dtype=torch.float)\n",
        "category_tensor=category_tensor.to(device)\n",
        "output = rnn(line_tensor)\n",
        "output=output.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NTAdeslYzLV",
        "outputId": "ea775a83-b4d5-435c-b867-4806ab5cdc6f"
      },
      "source": [
        "#testing how to get the data from the hd5 files\n",
        "import h5py\n",
        "\n",
        "hdf5_fpath=\"data.hdf5\"\n",
        "hdf5_file=h5py.File(hdf5_fpath, 'r')\n",
        "train_grp=hdf5_file[\"train\"]\n",
        "dev_keys=list(train_grp.keys())[:5000]\n",
        "train_keys=list(train_grp.keys())[5000:]\n",
        "test_grp=hdf5_file[\"test\"]\n",
        "test_keys=list(test_grp.keys())\n",
        "\n",
        "print(\"number of training items:\", len(train_keys))\n",
        "print(\"dev_set size:\",len(dev_keys))\n",
        "print(\"number of test items:\", len(test_keys))\n",
        "\n",
        "# for it in train_grp.items():\n",
        "#   print(it, it[1].attrs[\"text\"])\n",
        "\n",
        "  #print(dir(it[1]))\n",
        "#print(dir(train_grp))\n",
        "#print()\n",
        "#print(hdf5_file.items())\n",
        "hdf5_file.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training items: 28209\n",
            "dev_set size: 5000\n",
            "number of test items: 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6drIb1Xmvwc",
        "outputId": "5c929282-bcb1-4fd8-9b64-54e776a1eebc"
      },
      "source": [
        "#Clone the github codebase for aravec\n",
        "import os\n",
        "if os.path.exists(\"aravec\"): os.removedirs(\"aravec\")\n",
        "#!rm -r aravec #first delete the code_utils folder, then clone from the codebase (very crude till we figure out using github from colab)\n",
        "!git clone https://github.com/bakrianoo/aravec.git aravec\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'aravec'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Total 142 (delta 0), reused 0 (delta 0), pack-reused 142\u001b[K\n",
            "Receiving objects: 100% (142/142), 1.20 MiB | 11.69 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEb3-QP-nezl",
        "outputId": "a1c313c1-779e-4563-a673-2c950d8c99e9"
      },
      "source": [
        "!wget https://bakrianoo.s3-us-west-2.amazonaws.com/aravec/full_uni_cbow_300_twitter.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-29 01:38:56--  https://bakrianoo.s3-us-west-2.amazonaws.com/aravec/full_uni_cbow_300_twitter.zip\n",
            "Resolving bakrianoo.s3-us-west-2.amazonaws.com (bakrianoo.s3-us-west-2.amazonaws.com)... 52.218.233.57\n",
            "Connecting to bakrianoo.s3-us-west-2.amazonaws.com (bakrianoo.s3-us-west-2.amazonaws.com)|52.218.233.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2833686412 (2.6G) [application/zip]\n",
            "Saving to: ‘full_uni_cbow_300_twitter.zip’\n",
            "\n",
            "full_uni_cbow_300_t 100%[===================>]   2.64G  30.3MB/s    in 97s     \n",
            "\n",
            "2021-03-29 01:40:33 (27.8 MB/s) - ‘full_uni_cbow_300_twitter.zip’ saved [2833686412/2833686412]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pdi64RHnywK",
        "outputId": "937244b1-9d19-4476-974d-824b979aa109"
      },
      "source": [
        "!unzip full_uni_cbow_300_twitter.zip"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  full_uni_cbow_300_twitter.zip\n",
            "  inflating: full_uni_cbow_300_twitter.mdl  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OiEqH86pquv"
      },
      "source": [
        "import gensim\n",
        "t_model = gensim.models.Word2Vec.load('full_uni_cbow_300_twitter.mdl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frDcgfbvqg9z",
        "outputId": "6abbc146-679b-4350-b27f-e7ee49e87373"
      },
      "source": [
        "token=\"الحين\"\n",
        "most_similar = t_model.wv.most_similar( token, topn=10 )\n",
        "for term, score in most_similar:\n",
        "    print(term, score)\n",
        "print(t_model[token])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "اللحين 0.9532021880149841\n",
            "والحين 0.7758309841156006\n",
            "احين 0.7538288831710815\n",
            "هالحين 0.7515158653259277\n",
            "ذحين 0.7432119846343994\n",
            "للحين 0.7227292060852051\n",
            "خخ 0.7090663313865662\n",
            "دحين 0.7004846334457397\n",
            "بعدين 0.6678630113601685\n",
            "الحيين 0.6590395569801331\n",
            "[-5.39580643e-01 -2.04826093e+00  8.97866905e-01 -7.31150985e-01\n",
            " -1.73954618e+00  1.60691619e+00 -4.47108299e-01  2.23364145e-01\n",
            "  4.74283636e-01 -3.82652432e-01 -5.82857907e-01 -1.82154012e+00\n",
            "  1.59363902e+00 -3.91100824e-01 -1.13415682e+00 -9.23327327e-01\n",
            "  2.40160418e+00  7.57520497e-01 -2.47649050e+00  2.27496552e+00\n",
            "  9.63017046e-01  1.03695735e-01 -1.95540929e+00  2.17612967e-01\n",
            " -3.01175956e-02  5.87216020e-01  8.87227416e-01  6.85424864e-01\n",
            " -2.84710860e+00  3.86379504e+00  1.89357913e+00  9.91025090e-01\n",
            "  1.64410770e+00  1.73927629e+00  2.16164804e+00  6.65032715e-02\n",
            " -1.91086078e+00 -9.18030024e-01 -2.53077698e+00 -2.38934255e+00\n",
            " -1.60595167e+00 -1.31678939e-01  6.78059638e-01 -4.64854777e-01\n",
            " -3.97334844e-01  9.75031316e-01  2.04697156e+00  2.53835410e-01\n",
            "  1.52472651e+00 -2.42422891e+00 -3.51118803e-01  7.84251630e-01\n",
            "  1.99993742e+00 -1.39592743e+00  1.30688056e-01  5.51947057e-01\n",
            "  2.24000305e-01  3.03151131e+00  4.89707738e-01 -2.39840299e-01\n",
            " -1.15461206e+00 -2.54962301e+00  8.11143279e-01 -1.68599796e+00\n",
            " -1.28675580e+00 -1.15467131e+00  2.25439280e-01 -2.86397636e-01\n",
            "  1.08738542e-01  1.44101369e+00  7.41086841e-01 -4.95031685e-01\n",
            " -2.60768342e+00 -6.82549417e-01 -5.97893357e-01 -1.60995102e+00\n",
            " -4.81061906e-01 -5.97385943e-01  8.15488756e-01  4.57236975e-01\n",
            "  7.39545524e-01 -1.92395434e-01  1.14109123e+00  2.34028292e+00\n",
            " -1.40072191e+00  1.01170540e+00 -1.11009979e+00 -4.27468956e-01\n",
            " -1.42200327e+00  2.65230089e-01 -2.52453327e+00  6.91746250e-02\n",
            "  9.27411616e-01  8.19520473e-01  4.16955382e-01 -1.41449845e+00\n",
            " -6.12202704e-01 -2.25478649e+00 -1.61036754e+00 -1.28941309e+00\n",
            "  8.59458923e-01  3.52780610e-01  3.44272637e+00  2.88146305e+00\n",
            "  2.02922153e+00  2.06527591e+00 -7.38892734e-01 -6.57289550e-02\n",
            "  2.13767242e+00  1.46812665e+00 -4.55181271e-01  1.39931023e-01\n",
            " -1.24977279e+00  1.87827456e+00  8.19139257e-02  8.50859046e-01\n",
            " -4.22171772e-01  6.91716254e-01  5.95198810e-01 -1.88482356e+00\n",
            " -1.27515221e+00  2.47584224e+00  8.06555390e-01 -5.19083664e-02\n",
            "  2.04043269e+00  4.96099383e-01 -1.86842799e+00 -9.42114070e-02\n",
            " -1.28135598e+00  1.21727049e+00 -1.43805385e+00  4.71888334e-02\n",
            "  4.08165842e-01 -5.65551460e-01 -1.71714544e+00  1.06064296e+00\n",
            "  7.76017785e-01 -4.03799385e-01  6.44498229e-01  8.79206896e-01\n",
            " -4.14050430e-01 -9.24857080e-01  9.22883451e-01  7.96889663e-01\n",
            "  9.65920210e-01 -1.92071259e-01 -1.28274560e+00  5.37732244e-01\n",
            " -1.37313294e+00 -1.62182975e+00 -9.55344200e-01  2.29215336e+00\n",
            " -1.22887433e+00 -9.61196721e-01  1.47919029e-01  7.49479473e-01\n",
            "  3.00440693e+00 -1.25486159e+00  1.87144613e+00 -1.14503253e+00\n",
            "  6.36430562e-01 -1.74195778e+00  3.07420468e+00  1.79505914e-01\n",
            " -8.17050159e-01  8.97766531e-01 -1.56468201e+00 -1.05847943e+00\n",
            " -1.08474052e+00  7.50566721e-01  4.47499841e-01 -8.43635440e-01\n",
            " -1.74631786e+00 -9.28273141e-01 -1.71680224e+00 -8.23138833e-01\n",
            "  4.68580127e-01  2.47853541e+00  6.75132906e-04  1.02554250e+00\n",
            " -1.33838308e+00 -1.06877625e+00 -2.35732007e+00  5.39587557e-01\n",
            "  2.11787748e+00  8.95752251e-01  1.58882260e+00  1.38294411e+00\n",
            " -5.20562470e-01 -5.82600355e-01  7.99893260e-01 -3.47490907e-01\n",
            "  3.54181409e+00 -9.42439772e-04  8.00230145e-01  3.91613007e-01\n",
            "  2.33459210e+00 -1.39267647e+00  1.07977128e+00 -2.56835866e+00\n",
            " -1.64873526e-01  1.21521735e+00 -1.11308861e+00  1.08462000e+00\n",
            "  4.60371822e-01  7.27004588e-01  8.98043633e-01  1.05428779e+00\n",
            " -2.52954751e-01 -9.90724027e-01 -6.29362583e-01 -3.67536157e-01\n",
            " -7.59188712e-01  3.90189260e-01  5.67703903e-01  1.60096645e+00\n",
            "  6.79711640e-01 -2.99770880e+00  3.88657123e-01  1.06905532e+00\n",
            "  1.55155629e-01 -8.22017074e-01  5.01862764e-01 -2.56138277e+00\n",
            "  1.75399989e-01 -1.75636125e+00 -2.20795169e-01 -2.40318465e+00\n",
            " -2.81232912e-02  9.68873739e-01  8.50624144e-01 -8.95634055e-01\n",
            " -9.59676579e-02 -1.91204906e+00 -1.42037499e+00  8.78668368e-01\n",
            "  2.62626946e-01  1.38075024e-01  1.85482070e-01 -1.97675064e-01\n",
            " -1.23709607e+00 -2.77911406e-02 -1.26505589e+00 -3.86804342e-01\n",
            " -4.06157225e-03 -5.50125003e-01 -2.38366938e+00 -1.99058187e+00\n",
            " -4.42871451e-02  9.99890089e-01  2.22108722e+00 -1.43883860e+00\n",
            "  3.32549930e-01 -5.43224514e-01  1.20455265e+00  3.31702479e-03\n",
            " -9.69531476e-01 -4.83287126e-01  6.02462411e-01 -1.60960031e+00\n",
            "  4.79838997e-01 -4.44874763e-01  1.71705925e+00  4.28425789e-01\n",
            " -2.51797175e+00  1.17548597e+00  9.34981823e-01  1.99794984e+00\n",
            "  6.17137551e-01 -9.98692572e-01 -1.30828559e+00 -2.89432096e+00\n",
            " -2.00754952e+00  1.32005942e+00 -1.92859840e+00 -1.49523342e+00\n",
            " -1.12985432e+00 -1.89614773e+00  2.32920721e-01  6.30416572e-01\n",
            " -4.33334976e-01  2.32270646e+00  1.65665889e+00 -2.82243943e+00\n",
            "  1.59540391e+00  1.55336547e+00  5.01456976e-01  1.90422761e+00\n",
            "  8.21595728e-01  4.69825789e-02 -9.56614912e-01  1.14065349e+00\n",
            " -3.17781895e-01 -1.47116888e+00  1.91362154e+00  3.07885599e+00\n",
            " -1.30199254e+00  9.08842981e-01  1.90652013e+00  5.98181710e-02]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a70JbzaPq2IJ",
        "outputId": "cc880238-3cd0-4800-96a5-f836d5a9bff5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
        "\n",
        "#data = pd.read_excel(r\"C:\\Users\\ibrom\\Desktop\\NOTEBOOK\\NLP PRACICE\\AJGT.xlsx\")\n",
        "\n",
        "#data = pd.read_table(\"combined_training.tsv\")\n",
        "data = pd.read_csv(\"combined_training_copy.tsv\", sep='\\t')\n",
        "print(\"head\", data.head())\n",
        "print(data.sample(5))\n",
        "\n",
        "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
        "\n",
        "# Arabic stop words with nltk\n",
        "stop_words = stopwords.words()\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Shadda\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "def preprocess(text):\n",
        "    \n",
        "    '''\n",
        "    text is an arabic string input\n",
        "    \n",
        "    the preprocessed text is returned\n",
        "    '''\n",
        "    \n",
        "    #remove punctuations\n",
        "    translator = str.maketrans('', '', punctuations)\n",
        "    text = text.translate(translator)\n",
        "    \n",
        "    # remove Tashkeel\n",
        "    text = re.sub(arabic_diacritics, '', text)\n",
        "    \n",
        "    #remove longation\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "    return text\n",
        "  \n",
        "data['Feed'] = data['Feed'].apply(preprocess)\n",
        "print(data.head(5))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "head              ID Sentiment                                               Feed\n",
            "0  1.221875e+18  Positive  @nas_alharbi8 والله حسب الأرقام سيكون مخيب للآ...\n",
            "1  1.221884e+18   Neutral  الزعل بيغير ملامحك ، بيغير نظرة العين ، بيغير ...\n",
            "2  1.226423e+18  Positive  الحب الحقيقي هو اقتسام بعض نفسك مع شخص أخر أقر...\n",
            "3  1.221881e+18  Positive                          @Mo_Fat7 النهضة في فتيل 😂\n",
            "4  1.221884e+18   Neutral  @halgawi @DmfMohe ليس حباً في ايران بقدر ماهو ...\n",
            "                 ID  ...                                               Feed\n",
            "6464   1.182925e+18  ...  @ALAKBArKWWT مالومكم والله .. <br>اكيد زعلانين...\n",
            "19134  1.221784e+18  ...  أقوى قوة ممكن أن تملكها<br>هي وجودك بأفكارك في...\n",
            "14302  1.215176e+18  ...  @STCcare_KSA السلام عليكم عندي شريحه زين<br> ا...\n",
            "8761   1.146669e+18  ...  انتي ايش اللي يريحك تبي تجي عريانه يعني ؟ http...\n",
            "6048   1.223399e+18  ...  هيجى عليك وقت هتكون شخص قاسي في نظر البعض لمجر...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "             ID Sentiment                                               Feed\n",
            "0  1.221875e+18  Positive  nasalharbi8 والله حسب الارقام سيكون مخيب للاما...\n",
            "1  1.221884e+18   Neutral  الزعل بيغير ملامحك بيغير نظره العين بيغير شكلك...\n",
            "2  1.226423e+18  Positive  الحب الحقيقي اقتسام نفسك شخص اخر اقرب اليك نفس...\n",
            "3  1.221881e+18  Positive                               MoFat7 النهضه فتيل 😂\n",
            "4  1.221884e+18   Neutral  halgawi DmfMohe حبا ايران بقدر ماهو نكايه بترا...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJSId0s8D45V",
        "outputId": "2fc2d128-a05f-4cc2-b642-df56c481ca96"
      },
      "source": [
        "# splitting the data into target and feature\n",
        "feature = data.Feed\n",
        "target = data.Sentiment\n",
        "# splitting into train and tests\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n",
        "\n",
        "# make pipeline\n",
        "pipe = make_pipeline(TfidfVectorizer(),\n",
        "                    LogisticRegression())\n",
        "# make param grid\n",
        "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# create and fit the model\n",
        "lr_model = GridSearchCV(pipe, param_grid, cv=5)\n",
        "lr_model.fit(X_train,Y_train)\n",
        "\n",
        "# make prediction and print accuracy\n",
        "prediction = lr_model.predict(X_test)\n",
        "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
        "print(classification_report(Y_test, prediction))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is 0.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.63      0.24      0.35       982\n",
            "     Neutral       0.76      0.95      0.84      4549\n",
            "    Positive       0.62      0.27      0.38       989\n",
            "\n",
            "    accuracy                           0.74      6520\n",
            "   macro avg       0.67      0.49      0.53      6520\n",
            "weighted avg       0.72      0.74      0.70      6520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "RiwVopDxFNYA",
        "outputId": "cede70e3-c42d-444e-bb6b-8404398e6298"
      },
      "source": [
        "test_df = pd.read_csv(\"test1_with_text.csv\")\n",
        "\n",
        "test_df[\"Text\"] = test_df[\"Text\"].apply(preprocess)\n",
        "test_df.head(5)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet_id</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1251608623798722560</td>\n",
              "      <td>kamnapp bipksa nazahagovsa MCgovSA شوفوا هالاج...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1221535162539872256</td>\n",
              "      <td>ربما الموت يقترب مني وانا اشعر لطفك الله سكره ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1252203833385779200</td>\n",
              "      <td>mhrsdcare السلام عليكم لوسمحت تحديث ملف المنشا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1221884276636938240</td>\n",
              "      <td>لقد نام تاركا حلمه وداءع الله فايقظه الله علي ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1252686719461273602</td>\n",
              "      <td>nahdihope طالبه طلبيه منكم امس ولحد الان ماوصل...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Tweet_id                                               Text\n",
              "0  1251608623798722560  kamnapp bipksa nazahagovsa MCgovSA شوفوا هالاج...\n",
              "1  1221535162539872256  ربما الموت يقترب مني وانا اشعر لطفك الله سكره ...\n",
              "2  1252203833385779200  mhrsdcare السلام عليكم لوسمحت تحديث ملف المنشا...\n",
              "3  1221884276636938240  لقد نام تاركا حلمه وداءع الله فايقظه الله علي ...\n",
              "4  1252686719461273602  nahdihope طالبه طلبيه منكم امس ولحد الان ماوصل..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD5ndbCRJfWY",
        "outputId": "0e2bc031-0453-4962-d309-5d445237432f"
      },
      "source": [
        "test_df[\"Prediction\"] = lr_model.predict(test_df[\"Text\"])\n",
        "test_list=list(test_df[\"Prediction\"])\n",
        "print(len([v for v in test_list if v==\"Negative\"]))\n",
        "prediction_list=[]\n",
        "for i,a in enumerate(test_df.iterrows()):\n",
        "  if i%100==0: print(\"test item\", i)\n",
        "  row_dict=dict(iter(a[1].items())) \n",
        "  tweet_id=str(row_dict[\"Tweet_id\"])\n",
        "  tweet_text=row_dict[\"Text\"]\n",
        "  cur_pred=row_dict[\"Prediction\"]\n",
        "  top_pred=0\n",
        "  if cur_pred==\"Positive\": top_pred=1\n",
        "  if cur_pred==\"Negative\": top_pred=-1\n",
        "\n",
        "  #print(cur_pred, tweet_id)\n",
        "\n",
        "  #top_pred=cur_pred_list[0][0]\n",
        "\n",
        "  prediction_list.append((tweet_id,top_pred))\n",
        "\n",
        "\n",
        "\n",
        "import csv\n",
        "cur_fname=\"submission10sk.csv\"  \n",
        "# field names \n",
        "fields = ['Tweet_id', 'sentiment']    \n",
        "# data rows of csv file \n",
        "rows= prediction_list\n",
        "with open(cur_fname, 'w') as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerow(fields)\n",
        "    write.writerows(rows)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1394\n",
            "test item 0\n",
            "test item 100\n",
            "test item 200\n",
            "test item 300\n",
            "test item 400\n",
            "test item 500\n",
            "test item 600\n",
            "test item 700\n",
            "test item 800\n",
            "test item 900\n",
            "test item 1000\n",
            "test item 1100\n",
            "test item 1200\n",
            "test item 1300\n",
            "test item 1400\n",
            "test item 1500\n",
            "test item 1600\n",
            "test item 1700\n",
            "test item 1800\n",
            "test item 1900\n",
            "test item 2000\n",
            "test item 2100\n",
            "test item 2200\n",
            "test item 2300\n",
            "test item 2400\n",
            "test item 2500\n",
            "test item 2600\n",
            "test item 2700\n",
            "test item 2800\n",
            "test item 2900\n",
            "test item 3000\n",
            "test item 3100\n",
            "test item 3200\n",
            "test item 3300\n",
            "test item 3400\n",
            "test item 3500\n",
            "test item 3600\n",
            "test item 3700\n",
            "test item 3800\n",
            "test item 3900\n",
            "test item 4000\n",
            "test item 4100\n",
            "test item 4200\n",
            "test item 4300\n",
            "test item 4400\n",
            "test item 4500\n",
            "test item 4600\n",
            "test item 4700\n",
            "test item 4800\n",
            "test item 4900\n",
            "test item 5000\n",
            "test item 5100\n",
            "test item 5200\n",
            "test item 5300\n",
            "test item 5400\n",
            "test item 5500\n",
            "test item 5600\n",
            "test item 5700\n",
            "test item 5800\n",
            "test item 5900\n",
            "test item 6000\n",
            "test item 6100\n",
            "test item 6200\n",
            "test item 6300\n",
            "test item 6400\n",
            "test item 6500\n",
            "test item 6600\n",
            "test item 6700\n",
            "test item 6800\n",
            "test item 6900\n",
            "test item 7000\n",
            "test item 7100\n",
            "test item 7200\n",
            "test item 7300\n",
            "test item 7400\n",
            "test item 7500\n",
            "test item 7600\n",
            "test item 7700\n",
            "test item 7800\n",
            "test item 7900\n",
            "test item 8000\n",
            "test item 8100\n",
            "test item 8200\n",
            "test item 8300\n",
            "test item 8400\n",
            "test item 8500\n",
            "test item 8600\n",
            "test item 8700\n",
            "test item 8800\n",
            "test item 8900\n",
            "test item 9000\n",
            "test item 9100\n",
            "test item 9200\n",
            "test item 9300\n",
            "test item 9400\n",
            "test item 9500\n",
            "test item 9600\n",
            "test item 9700\n",
            "test item 9800\n",
            "test item 9900\n",
            "test item 10000\n",
            "test item 10100\n",
            "test item 10200\n",
            "test item 10300\n",
            "test item 10400\n",
            "test item 10500\n",
            "test item 10600\n",
            "test item 10700\n",
            "test item 10800\n",
            "test item 10900\n",
            "test item 11000\n",
            "test item 11100\n",
            "test item 11200\n",
            "test item 11300\n",
            "test item 11400\n",
            "test item 11500\n",
            "test item 11600\n",
            "test item 11700\n",
            "test item 11800\n",
            "test item 11900\n",
            "test item 12000\n",
            "test item 12100\n",
            "test item 12200\n",
            "test item 12300\n",
            "test item 12400\n",
            "test item 12500\n",
            "test item 12600\n",
            "test item 12700\n",
            "test item 12800\n",
            "test item 12900\n",
            "test item 13000\n",
            "test item 13100\n",
            "test item 13200\n",
            "test item 13300\n",
            "test item 13400\n",
            "test item 13500\n",
            "test item 13600\n",
            "test item 13700\n",
            "test item 13800\n",
            "test item 13900\n",
            "test item 14000\n",
            "test item 14100\n",
            "test item 14200\n",
            "test item 14300\n",
            "test item 14400\n",
            "test item 14500\n",
            "test item 14600\n",
            "test item 14700\n",
            "test item 14800\n",
            "test item 14900\n",
            "test item 15000\n",
            "test item 15100\n",
            "test item 15200\n",
            "test item 15300\n",
            "test item 15400\n",
            "test item 15500\n",
            "test item 15600\n",
            "test item 15700\n",
            "test item 15800\n",
            "test item 15900\n",
            "test item 16000\n",
            "test item 16100\n",
            "test item 16200\n",
            "test item 16300\n",
            "test item 16400\n",
            "test item 16500\n",
            "test item 16600\n",
            "test item 16700\n",
            "test item 16800\n",
            "test item 16900\n",
            "test item 17000\n",
            "test item 17100\n",
            "test item 17200\n",
            "test item 17300\n",
            "test item 17400\n",
            "test item 17500\n",
            "test item 17600\n",
            "test item 17700\n",
            "test item 17800\n",
            "test item 17900\n",
            "test item 18000\n",
            "test item 18100\n",
            "test item 18200\n",
            "test item 18300\n",
            "test item 18400\n",
            "test item 18500\n",
            "test item 18600\n",
            "test item 18700\n",
            "test item 18800\n",
            "test item 18900\n",
            "test item 19000\n",
            "test item 19100\n",
            "test item 19200\n",
            "test item 19300\n",
            "test item 19400\n",
            "test item 19500\n",
            "test item 19600\n",
            "test item 19700\n",
            "test item 19800\n",
            "test item 19900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "cGLGSwgJJ6bF",
        "outputId": "acabd321-b9f1-49f6-8868-8c13d5142c09"
      },
      "source": [
        "pipe = make_pipeline(TfidfVectorizer(),\n",
        "                    RandomForestClassifier())\n",
        "\n",
        "param_grid = {'randomforestclassifier__n_estimators':[10, 100, 1000],\n",
        "             'randomforestclassifier__max_features':['sqrt', 'log2']}\n",
        "\n",
        "rf_model = GridSearchCV(pipe, param_grid, cv=5)\n",
        "rf_model.fit(X_train,Y_train)\n",
        "\n",
        "prediction = rf_model.predict(X_test)\n",
        "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-85493f9130d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3fBFwZCKKaY",
        "outputId": "f9c4a0f0-f705-4ab6-a260-8fabdc035fe6"
      },
      "source": [
        "pipe = make_pipeline(TfidfVectorizer(),\n",
        "                    MultinomialNB())\n",
        "pipe.fit(X_train,Y_train)\n",
        "prediction = pipe.predict(X_test)\n",
        "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
        "print(classification_report(Y_test, prediction))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score is 0.70\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.93      0.01      0.03       982\n",
            "     Neutral       0.70      1.00      0.82      4549\n",
            "    Positive       0.77      0.01      0.02       989\n",
            "\n",
            "    accuracy                           0.70      6520\n",
            "   macro avg       0.80      0.34      0.29      6520\n",
            "weighted avg       0.75      0.70      0.58      6520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXhsEbvsRU8W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}